from __future__ import annotations

import json
import textwrap
import logging
from typing import List, Optional, Dict, Any, Literal, TypedDict
from concurrent.futures import ThreadPoolExecutor
from datetime import timedelta

from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers.pydantic import PydanticOutputParser
from langgraph.graph import StateGraph, END

from langchain_google_genai import ChatGoogleGenerativeAI
from app.core.config import settings
from app.database.promotion_slots import update_state
from app.agents.promotion.state import get_action_state
from .state import *
from .tools import *
from .helpers import *

logger = logging.getLogger(__name__)

# ===== Nodes =====

def _merge_slots(state: OrchestratorState, updates: Dict[str, Any]) -> PromotionSlots:
    current = (state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots())
    base = current.model_dump()
    for k, v in updates.items():
        if v not in (None, "", []):
            base[k] = v
    merged = PromotionSlots(**base)
    if state.get("active_task"):
        state["active_task"].slots = merged
    return merged

def slot_extractor_node(state: OrchestratorState):
    logger.info("--- 2. ìŠ¬ë¡¯ ì¶”ì¶œ/ì €ì¥ ë…¸ë“œ ì‹¤í–‰ ---")
    user_message = state.get("user_message", "")
    thread_id = state["thread_id"]

    parser = PydanticOutputParser(pydantic_object=PromotionSlotUpdate)
    prompt_tmpl = textwrap.dedent("""
    ì•„ë˜ í•œêµ­ì–´ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ **í”„ë¡œëª¨ì…˜ ìŠ¬ë¡¯ ê°’**ì„ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.
    - ì¡´ì¬í•˜ëŠ” ê°’ë§Œ ì±„ìš°ê³ , ì—†ìœ¼ë©´ nullë¡œ ë‘ì„¸ìš”.
    - target_typeì€ "brand_target" ë˜ëŠ” "category_target" ì¤‘ í•˜ë‚˜ë¡œë§Œ.
    - ë‚ ì§œ/ê¸°ê°„ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë¬¸ìì—´ë¡œ ìœ ì§€.
    - ì¶œë ¥ì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ì„¸ìš”:
      {format_instructions}

    [ì‚¬ìš©ì ë©”ì‹œì§€]
    {user_message}
    """)
    prompt = ChatPromptTemplate.from_template(
        prompt_tmpl,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        model_kwargs={"response_format": {"type": "json_object"}},
        api_key=settings.GOOGLE_API_KEY
    )
    parsed: PromotionSlotUpdate = (prompt | llm | parser).invoke({"user_message": user_message})

    updates = {k: v for k, v in parsed.model_dump().items() if v not in (None, "", [])}
    if not updates:
        logger.info("ìŠ¬ë¡¯ ì—…ë°ì´íŠ¸ ì—†ìŒ")
        return {}

    try:
        update_state(thread_id, updates)
        logger.info("Mongo ìƒíƒœ ì—…ë°ì´íŠ¸: %s", updates)
    except Exception as e:
        logger.error("Mongo ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: %s", e)

    merged = _merge_slots(state, updates)
    logger.info("State ìŠ¬ë¡¯ ë³‘í•©: %s", merged.model_dump())
    return {"tool_results": {"slot_updates": updates}}

def _planner_router(state: OrchestratorState) -> str:
    instr = state.get("instructions")
    if not instr:
        return "tool_executor"
    resp = (instr.response_generator_instruction or "").strip()
    if resp.startswith("[PROMOTION]"):
        return "slot_extractor"
    if instr.t2s_instruction or instr.knowledge_instruction:
        return "tool_executor"
    return "response_generator"

def planner_node(state: OrchestratorState):
    logger.info("--- 1. ğŸ¤” ê³„íš ìˆ˜ë¦½ ë…¸ë“œ ì‹¤í–‰ ---")

    parser = PydanticOutputParser(pydantic_object=OrchestratorInstruction)
    history_summary = summarize_history(state.get("history", []))
    active_task_dump = state['active_task'].model_dump_json() if state.get('active_task') else 'null'
    schema_sig = state.get("schema_info", "")
    today = today_kr()

    prompt_template = textwrap.dedent("""
    You are the orchestrator for a marketing agent. Decide what to do this turn using ONLY the provided context.
    You MUST output a JSON that strictly follows: {format_instructions}

    ## Route decision (VERY IMPORTANT)
    - Decide the user's intent as one of:
      - Promotion flow (create/continue a promotion)
      - One-off answer (DB facts via t2s, or knowledge snippet)
      - Out-of-scope guidance
    - If and only if it is **promotion flow**, prefix `response_generator_instruction` with "[PROMOTION]".
    - If it is **out-of-scope guidance**, prefix with "[OUT_OF_SCOPE]".
    - Otherwise (one-off answer), no prefix.

    ## Tools
    - t2s: use for DB aggregations/rankings/trends.
    - knowledge: use for external trend summaries. Do not invent DB facts.

    ## Time normalization
    - Convert relative dates to ABSOLUTE ranges with Asia/Seoul timezone. Today is {today}.

    ## DB schema signature (hint only):
    {schema_sig}

    ## Conversation summary (last turns):
    {history_summary}

    ## Active task snapshot (JSON or null):
    {active_task}

    ## Decision rules
    - Promotion flow: do NOT call tools this turn. Just set `response_generator_instruction` (with [PROMOTION]).
    - One-off answers: set `t2s_instruction` and/or `knowledge_instruction` as needed.
    - Out-of-scope: both tools null, and provide short polite guidance with [OUT_OF_SCOPE].
    - Output must be concise, Korean polite style.

    User Message: "{user_message}"
    """)

    prompt = ChatPromptTemplate.from_template(
        template=prompt_template,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        model_kwargs={"response_format": {"type": "json_object"}},
        api_key=settings.GOOGLE_API_KEY
    )

    instructions = (prompt | llm | parser).invoke({
        "user_message": state['user_message'],
        "history_summary": history_summary,
        "active_task": active_task_dump,
        "schema_sig": schema_sig,
        "today": today,
    })

    return {"instructions": instructions}

def action_state_node(state: OrchestratorState):
    logger.info("--- 3. ğŸ“‹ ì•¡ì…˜ ìƒíƒœ í™•ì¸ ë…¸ë“œ ì‹¤í–‰ ---")
    slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else None
    decision = get_action_state(slots=slots)
    logger.info("Action decision: %s", decision)
    return {"tool_results": {"action": decision}}

def _action_router(state: OrchestratorState) -> str:
    """
    action_state ê²°ê³¼ë¡œ ë‹¤ìŒ ë…¸ë“œ ê²°ì •:
    - brand/targetì„ ë¬»ëŠ” ìƒí™©ì´ë©´ options_generator
    - ê·¸ ì™¸ ask_for_slots/objective/durationì€ response_generator
    - start_promotion/skipë„ response_generatorë¡œ (LLMì´ ìš”ì•½/ì•ˆë‚´)
    """
    tr = state.get("tool_results") or {}
    action = tr.get("action") or {}
    status = action.get("status")
    missing = action.get("missing_slots", [])

    if status == "ask_for_slots" and any(m in ("brand", "target") for m in missing):
        return "options_generator"
    return "response_generator"

def _build_candidate_t2s_instruction(target_type: str, lookback_days: int = 60) -> str:
    end = datetime.now(ZoneInfo("Asia/Seoul")).date()
    start = end - timedelta(days=lookback_days)
    # í‘œì¤€ alias ê°•ì œ
    if target_type == "brand_target":
        return textwrap.dedent(f"""
        ìµœê·¼ ê¸°ê°„ {start}~{end}ì™€ ì§ì „ ë™ì¼ ê¸°ê°„ì„ ë¹„êµí•˜ì—¬ ë¸Œëœë“œ ë ˆë²¨ í›„ë³´ ëª©ë¡ì„ ì‚°ì¶œí•´ ì£¼ì„¸ìš”.
        ë°˜ë“œì‹œ ë‹¤ìŒ ì»¬ëŸ¼ aliasë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
        - brand_name
        - revenue (ìµœê·¼ ê¸°ê°„ ë§¤ì¶œ)
        - growth_pct (ì´ì „ ë™ì¼ê¸°ê°„ ëŒ€ë¹„ ì¦ê°ìœ¨, %)
        - gm (ìµœê·¼ ê¸°ê°„ ì´ì´ìµë¥ , 0~1)
        - conversion_rate
        - repeat_rate
        - aov
        - inventory_days
        - return_rate
        - category_name
        - price_band
        - gender_age
        í–‰ì€ ë¸Œëœë“œë³„ 1í–‰ì…ë‹ˆë‹¤. ìµœê·¼ ê¸°ê°„ ë§¤ì¶œ ìƒìœ„ 100ê°œ ë‚´ì—ì„œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
        """).strip()
    else:
        # category/product ê´€ì 
        return textwrap.dedent(f"""
        ìµœê·¼ ê¸°ê°„ {start}~{end}ì™€ ì§ì „ ë™ì¼ ê¸°ê°„ì„ ë¹„êµí•˜ì—¬ ì¹´í…Œê³ ë¦¬/ìƒí’ˆ ë ˆë²¨ í›„ë³´ ëª©ë¡ì„ ì‚°ì¶œí•´ ì£¼ì„¸ìš”.
        ê°€ëŠ¥í•œ ê²½ìš° ë‹¤ìŒ ì»¬ëŸ¼ aliasë¥¼ í¬í•¨í•˜ì„¸ìš”:
        - product_id
        - product_name
        - category_name
        - revenue
        - growth_pct
        - gm
        - conversion_rate
        - repeat_rate
        - aov
        - inventory_days
        - return_rate
        - price_band
        - gender_age
        í–‰ì€ ìƒí’ˆ(ë˜ëŠ” ì¹´í…Œê³ ë¦¬)ë³„ 1í–‰ì…ë‹ˆë‹¤. ìµœê·¼ ê¸°ê°„ ë§¤ì¶œ ìƒìœ„ 200ê°œ ë‚´ì—ì„œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
        """).strip()

def options_generator_node(state: OrchestratorState):
    logger.info("--- 4. ğŸ§  ì˜µì…˜ ì œì•ˆ ë…¸ë“œ ì‹¤í–‰ ---")
    thread_id = state["thread_id"]
    slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots()
    target_type = slots.target_type or "brand_target"

    # 1) t2së¡œ í›„ë³´ ì§‘ê³„
    t2s_instr = _build_candidate_t2s_instruction(target_type)
    table = run_t2s_agent_with_instruction(state["conn_str"], state["schema_info"], t2s_instr)
    rows = table["rows"]

    if not rows:
        logger.warning("t2s í›„ë³´ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        update_state(thread_id, {"product_options": []})
        tr = state.get("tool_results") or {}
        tr["option_candidates"] = {"candidates": [], "method": "deterministic_v1", "time_window": "", "constraints": {}}
        return {"tool_results": tr}

    # 2) knowledge ìŠ¤ëƒ…ìƒ·
    knowledge = get_knowledge_snapshot()
    trending_terms = knowledge.get("trending_terms", [])

    # 3) ê¸°íšŒì ìˆ˜ ê³„ì‚° + ë‹¤ì–‘ì„± ì„ íƒ
    enriched = compute_opportunity_score(rows, trending_terms)
    topk = pick_diverse_top_k(enriched, k=4)

    # 4) í›„ë³´ JSON + ìƒíƒœ ì €ì¥
    # ë¼ë²¨ êµ¬ì„±: target_typeë³„ë¡œ ë‹¤ë¥´ê²Œ
    labels: List[str] = []
    candidates: List[Dict[str, Any]] = []
    for r in topk:
        if target_type == "brand_target":
            cid = f"brand:{r.get('brand_name')}"
            label = str(r.get("brand_name") or "ì•Œ ìˆ˜ ì—†ëŠ” ë¸Œëœë“œ")
            typ = "brand"
        else:
            name = r.get("product_name") or r.get("category_name") or "ì•Œ ìˆ˜ ì—†ëŠ” í•­ëª©"
            cid = f"product:{r.get('product_id') or name}"
            label = str(name)
            typ = "product" if r.get("product_name") else "category"
        labels.append(label)
        candidates.append({
            "id": cid,
            "label": label,
            "type": typ,
            "metrics": {k: r.get(k) for k in (
                "revenue","growth_pct","gm","conversion_rate","repeat_rate","aov","inventory_days","seasonality_score","return_rate"
            ) if k in r},
            "opportunity_score": r.get("opportunity_score"),
            "reasons": r.get("reasons", []),
            "diversity_tags": [x for x in (r.get("category_name"), r.get("price_band"), r.get("gender_age")) if x],
        })

    option_json = {
        "candidates": candidates,
        "method": "deterministic_v1",
        "time_window": "",
        "constraints": {"min_gm": 0.25, "max_return_rate": 0.1},
    }

    # Mongo: ë¼ë²¨ ì €ì¥
    try:
        update_state(thread_id, {"product_options": labels})
    except Exception as e:
        logger.error("ì˜µì…˜ ë¼ë²¨ ì €ì¥ ì‹¤íŒ¨: %s", e)

    # Stateì—ë„ ë°˜ì˜
    merged_slots = _merge_slots(state, {"product_options": labels})
    logger.info("ì˜µì…˜ ë¼ë²¨ state ë°˜ì˜: %s", merged_slots.product_options)

    # tool_results ì €ì¥
    tr = state.get("tool_results") or {}
    tr["option_candidates"] = option_json
    return {"tool_results": tr}

def tool_executor_node(state: OrchestratorState):
    logger.info("--- 5. ğŸ”¨ íˆ´ ì‹¤í–‰ ë…¸ë“œ ---")
    instructions = state.get("instructions")
    existing_results = dict(state.get("tool_results") or {})
    if not instructions or (not instructions.t2s_instruction and not instructions.knowledge_instruction):
        logger.info("í˜¸ì¶œí•  íˆ´ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {"tool_results": existing_results or None}

    tool_results: Dict[str, Any] = {}
    with ThreadPoolExecutor() as executor:
        futures = {}
        if instructions.t2s_instruction:
            futures[executor.submit(run_t2s_agent_with_instruction, state["conn_str"], state["schema_info"], instructions.t2s_instruction)] = "t2s"
        if instructions.knowledge_instruction:
            # knowledgeëŠ” ê°„ë‹¨ ìŠ¤í…: ë¬¸ìì—´/ìš”ì•½ ì •ë„ë§Œ
            futures[executor.submit(lambda: "ìµœê·¼ ìˆí¼ ì½˜í…ì¸ ë¥¼ í™œìš©í•œ ë°”ì´ëŸ´ ë§ˆì¼€íŒ…ì´ ì¸ê¸°ì…ë‹ˆë‹¤.")] = "knowledge"

        for future in futures:
            tool_name = list(futures.values())[list(futures.keys()).index(future)]
            try:
                tool_results[tool_name] = future.result()
            except Exception as e:
                logger.error("%s íˆ´ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: %s", tool_name, e)
                tool_results[tool_name] = {"error": str(e)}

    merged = {**existing_results, **tool_results} if existing_results else tool_results
    return {"tool_results": merged}

def response_generator_node(state: OrchestratorState):
    logger.info("--- 6. ğŸ—£ï¸ ì‘ë‹µ ìƒì„± ë…¸ë“œ ---")
    instructions = state.get("instructions")
    tr = state.get("tool_results") or {}

    instructions_text = (
        instructions.response_generator_instruction
        if instructions and instructions.response_generator_instruction
        else "ì‚¬ìš©ì ìš”ì²­ì— ë§ì¶° ì •ì¤‘í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    )

    t2s_table = tr.get("t2s") if isinstance(tr.get("t2s"), dict) else None
    action_decision = tr.get("action")
    knowledge_snippet = tr.get("knowledge") if isinstance(tr.get("knowledge"), str) else None
    option_candidates = tr.get("option_candidates") if isinstance(tr.get("option_candidates"), dict) else None

    prompt_tmpl = textwrap.dedent("""
    ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì˜ ìµœì¢… ì‘ë‹µ ìƒì„±ê¸°ì…ë‹ˆë‹¤.
    ì•„ë˜ ì…ë ¥ë§Œì„ ê·¼ê±°ë¡œ **í•œêµ­ì–´ ì¡´ëŒ“ë§**ë¡œ í•œ ë²ˆì— ì™„ì„±ëœ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    ë‚´ë¶€ ë„êµ¬ëª…ì´ë‚˜ ì‹œìŠ¤í…œ ì„¸ë¶€ êµ¬í˜„ì€ ì–¸ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    [ì…ë ¥ ì„¤ëª…]
    - instructions_text: ì´ë²ˆ í„´ì˜ í†¤/ë°©í–¥.
    - action_decision: í”„ë¡œëª¨ì…˜ ì˜ì‚¬ê²°ì •(JSON). statusì— ë”°ë¼ í•„ìš”í•œ ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”.
    - t2s_table: DB ì§ˆì˜ ê²°ê³¼(JSON). ìˆìœ¼ë©´ ìƒìœ„ 10í–‰ë§Œ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë¯¸ë¦¬ë³´ê¸°.
    - knowledge_snippet: ì™¸ë¶€ íŠ¸ë Œë“œ ìš”ì•½(ì„ íƒ).
    - option_candidates: ìœ ì €ì—ê²Œ ì œì•ˆí•  í›„ë³´ ëª©ë¡(JSON). ìˆìœ¼ë©´ **ë²ˆí˜¸ë¥¼ ë§¤ê²¨** ì œì‹œí•˜ê³ , ê° í•­ëª©ë‹¹ 1~2ì¤„ì˜ ê·¼ê±°ë¥¼ ë§ë¶™ì´ì„¸ìš”.
      ë§ˆì§€ë§‰ì— â€œê¸°íƒ€(ì§ì ‘ ì…ë ¥)â€ í•­ëª©ë„ í¬í•¨í•´ ì£¼ì„¸ìš”.

    [ì‘ì„± ì§€ì¹¨]
    1) instructions_textë¥¼ ìš°ì„ í•©ë‹ˆë‹¤.
    2) action_decision.status:
       - "ask_for_slots": ask_promptsë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë¬»ìŠµë‹ˆë‹¤.
       - "start_promotion": payload í•µì‹¬ë§Œ ì§§ê²Œ í™•ì¸í•©ë‹ˆë‹¤.
       - "skip"/ì—†ìŒ: ë¬´ì‹œí•©ë‹ˆë‹¤.
    3) option_candidatesê°€ ìˆìœ¼ë©´:
       - 1) 2) 3) ... ì²˜ëŸ¼ **ì„ íƒì§€ë¥¼ ë²ˆí˜¸ë¡œ** ì œì‹œí•˜ê³ , ê° í•­ëª© ì˜†ì— ì´ìœ (ê·¼ê±° ì§€í‘œ) 1~2ì¤„.
       - â€œê¸°íƒ€(ì§ì ‘ ì…ë ¥)â€ë„ ë§ˆì§€ë§‰ì— ë„£ìœ¼ì„¸ìš”.
       - ìœ ì €ê°€ ë²ˆí˜¸ë‚˜ ë¼ë²¨ë¡œ ë‹µí•´ë„ ëœë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”.
    4) t2s_tableì´ ìˆìœ¼ë©´ ìƒìœ„ 10í–‰ ë¯¸ë¦¬ë³´ê¸° í‘œë¥¼ í¬í•¨í•˜ë˜, ì—†ëŠ” ì‚¬ì‹¤ì€ ì°½ì‘í•˜ì§€ ë§ˆì„¸ìš”.
    5) knowledge_snippetì´ ìˆìœ¼ë©´ â€œì°¸ê³  íŠ¸ë Œë“œâ€ë¡œ 1~2ì¤„ ë§ë¶™ì´ì„¸ìš”.
    6) ê°„ê²°í•˜ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ì„ ìœ ì§€í•˜ì„¸ìš”.

    [ì…ë ¥ ë°ì´í„°]
    - instructions_text:
    {instructions_text}

    - action_decision (JSON):
    {action_decision_json}

    - t2s_table (JSON):
    {t2s_table_json}

    - option_candidates (JSON):
    {option_candidates_json}

    - knowledge_snippet:
    {knowledge_snippet}
    """)

    to_json = lambda x: json.dumps(x, ensure_ascii=False) if x is not None else "null"

    prompt = ChatPromptTemplate.from_template(prompt_tmpl)
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        api_key=settings.GOOGLE_API_KEY
    )

    final_text = (prompt | llm).invoke({
        "instructions_text": instructions_text,
        "action_decision_json": to_json(action_decision),
        "t2s_table_json": to_json(ensure_table_payload(t2s_table) if t2s_table else None),
        "option_candidates_json": to_json(option_candidates),
        "knowledge_snippet": knowledge_snippet or ""
    })

    final_response = getattr(final_text, "content", None) or str(final_text)
    logger.info(f"ìµœì¢… ê²°ê³¼(L):\n{final_response}")

    history = state.get("history", [])
    history.append({"role": "user", "content": state.get("user_message", "")})
    history.append({"role": "assistant", "content": final_response})

    return {"history": history, "user_message": "", "output": final_response}

# ===== Graph =====
workflow = StateGraph(OrchestratorState)

workflow.add_node("planner", planner_node)
workflow.add_node("slot_extractor", slot_extractor_node)
workflow.add_node("action_state", action_state_node)
workflow.add_node("options_generator", options_generator_node)
workflow.add_node("tool_executor", tool_executor_node)
workflow.add_node("response_generator", response_generator_node)

workflow.set_entry_point("planner")

workflow.add_conditional_edges(
    "planner",
    _planner_router,
    {
        "slot_extractor": "slot_extractor",
        "tool_executor": "tool_executor",
        "response_generator": "response_generator",
    },
)

workflow.add_edge("slot_extractor", "action_state")
workflow.add_conditional_edges(
    "action_state",
    _action_router,
    {
        "options_generator": "options_generator",
        "response_generator": "response_generator",
    },
)
workflow.add_edge("options_generator", "response_generator")

workflow.add_edge("tool_executor", "response_generator")
workflow.add_edge("response_generator", END)

orchestrator_app = workflow.compile()