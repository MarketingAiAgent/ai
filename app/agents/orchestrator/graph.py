import json
import textwrap
import logging
from typing import List
from concurrent.futures import ThreadPoolExecutor

from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers.pydantic import PydanticOutputParser
from langgraph.graph import StateGraph, END

from .state import *
from app.core.config import settings 
from app.agents.text_to_sql.__init__ import call_sql_generator

logger = logging.getLogger(__name__)

# --- íˆ´ í•¨ìˆ˜ ---
def run_t2s_agent(state: OrchestratorState):
    instruction = state['instructions'].t2s_instruction

    logger.info("T2S ì—ì´ì „íŠ¸ ì‹¤í–‰: %s", instruction)

    state = call_sql_generator(message=instruction, conn_str=state['conn_str'], schema_info=state['schema_info'])
    table = state.data_json

    if isinstance(table, str): 
        table = json.loads(table)

    return table

# ì¼ë‹¨ ëª¨ì¡°í’ˆë§Œ
def run_knowledge_agent(instruction: str) -> str:
    logger.info("ì§€ì‹ ì—ì´ì „íŠ¸ ì‹¤í–‰: %s", instruction)
    return "ìµœê·¼ ìˆí¼ ì½˜í…ì¸ ë¥¼ í™œìš©í•œ ë°”ì´ëŸ´ ë§ˆì¼€íŒ…ì´ ì¸ê¸°ì…ë‹ˆë‹¤."

# --- LangGraph ë…¸ë“œ ì •ì˜ ---
def planner_node(state: OrchestratorState):
    """ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."""

    logger.info("--- 1. ğŸ¤” ê³„íš ìˆ˜ë¦½ ë…¸ë“œ (Planner) ì‹¤í–‰ ---")

    parser = PydanticOutputParser(pydantic_object=OrchestratorInstruction)
    
    prompt_template = textwrap.dedent("""
    You are a master AI orchestrator. Your job is to analyze the user's request and the current conversation state, then create a complete, step-by-step plan for this turn.
    The plan must be a JSON object that follows this format: {format_instructions}

    # Rules:
    1.  **Analyze Task:** First, check if there is an `active_task` for 'promotion'. If not, and the user wants to start one, create a new task.
    2.  **Plan Tool Use:** Based on the user's message and the current `slots`, decide if you need to call tools (`t2s_agent` or `knowledge_agent`).
    3.  **Plan Response:** Based on the plan, create an instruction for the `response_generator`.

    # Current State & History:
    - User Message: "{user_message}"
    - Active Task: {active_task}
    - History: {history}
    """)
    
    prompt = ChatPromptTemplate.from_template(
        template=prompt_template,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0, model_kwargs={"response_format": {"type": "json_object"}}, api_key=settings.GOOGLE_API_KEY)
    chain = prompt | llm | parser

    instructions = chain.invoke({
        "user_message": state['user_message'],
        "active_task": state['active_task'].model_dump_json() if state.get('active_task') else 'None',
        "history": json.dumps(state['history'])
    })
    
    return {"instructions": instructions}

def tool_executor_node(state: OrchestratorState):
    """í•„ìš”í•œ íˆ´ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    logger.info("--- 2. ğŸ”¨ íˆ´ ì‹¤í–‰ ë…¸ë“œ (Tool Executor) ì‹¤í–‰ ---")
    
    instructions = state.get("instructions")
    if not instructions or (not instructions.t2s_instruction and not instructions.knowledge_instruction):
        logger.info("í˜¸ì¶œí•  íˆ´ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {"tool_results": None}

    tool_results = {}
    
    with ThreadPoolExecutor() as executor:
        futures = {}
        if instructions.t2s_instruction:
            futures[executor.submit(run_t2s_agent, instructions.t2s_instruction)] = "t2s"
        if instructions.knowledge_instruction:
            futures[executor.submit(run_knowledge_agent, instructions.knowledge_instruction)] = "knowledge"
        
        for future in futures:
            tool_name = futures[future]
            try:
                tool_results[tool_name] = future.result()
            except Exception as e:
                logger.error("%s íˆ´ ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: %s", tool_name, e)
                tool_results[tool_name] = f"Error: {e}"
    
    return {"tool_results": tool_results}

def response_generator_node(state: OrchestratorState):
    """ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ê³  íˆìŠ¤í† ë¦¬ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    logger.info("--- 3. ğŸ—£ï¸ ì‘ë‹µ ìƒì„± ë…¸ë“œ (Response Generator) ì‹¤í–‰ ---")
    
    instructions = state.get("instructions")
    tool_results = state.get("tool_results")
    
    response_instruction = instructions.response_generator_instruction
    final_response = f"[ì‘ë‹µ ìƒì„± ì§€ì‹œ: {response_instruction}]"
    
    if tool_results:
        final_response += f"\n\n[íˆ´ ì‹¤í–‰ ê²°ê³¼: {json.dumps(tool_results, ensure_ascii=False)}]"

    history = state.get("history", [])
    history.append({"role": "user", "content": state["user_message"]})
    history.append({"role": "assistant", "content": final_response})
    
    return {"history": history, "user_message": ""}

# --- ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼ ---
workflow = StateGraph(OrchestratorState)

workflow.add_node("planner", planner_node)
workflow.add_node("tool_executor", tool_executor_node)
workflow.add_node("response_generator", response_generator_node)

workflow.set_entry_point("planner")
workflow.add_edge("planner", "tool_executor")
workflow.add_edge("tool_executor", "response_generator")
workflow.add_edge("response_generator", END)

orchestrator_app = workflow.compile()