# app/agents/orchestrator/graph.py
from __future__ import annotations

import json
import textwrap
import logging
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo   
import time 
from typing import List, Optional, Dict, Any, Literal, TypedDict, Union
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from datetime import timedelta

from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers.pydantic import PydanticOutputParser
from langgraph.graph import StateGraph, END

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic

from app.core.config import settings
from app.database.promotion_slots import update_state
from app.agents.promotion.state import get_action_state
from app.agents.visualizer.graph import build_visualize_graph
from app.agents.visualizer.state import VisualizeState
from .state import *
from .tools import *
from .helpers import *

logger = logging.getLogger(__name__)

# ===== Helper (length policy) =====
def _compute_length_hint(tr: Dict[str, Any], option_candidates: Optional[Dict[str, Any]], t2s_table: Optional[Dict[str, Any]]) -> str:
    """ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±: í‘œ/ì˜µì…˜/ì™¸ë¶€ê·¼ê±° ìœ ë¬´ë¡œ ê¸¸ì´ ì¶”ì²œ"""
    has_table = bool(t2s_table and t2s_table.get("rows"))
    has_options = bool(option_candidates and option_candidates.get("candidates"))
    has_evidence = any(
        k in tr and tr.get(k)
        for k in ("marketing_trend_search_0", "tavily_search_0", "scrape_webpages_0", "beauty_youtuber_trend_search_0")
    )
    score = int(has_table) + int(has_options) + int(has_evidence)
    if score >= 2:
        return "long"    # 13~20ë¬¸ì¥
    if score == 1:
        return "medium"  # 7~12ë¬¸ì¥
    return "short"       # 4~6ë¬¸ì¥

# ===== P1-3: Deterministic ì˜µì…˜ ë Œë”ë§ =====
def _render_option_list_text(
    option_candidates: Optional[Dict[str, Any]],
    slots: Optional[PromotionSlots],
    *,
    max_items: int = 6,
) -> str:
    """
    í›„ë³´ JSONì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©í•˜ê³ ,
    ì—†ìœ¼ë©´ slots.product_options(ë¼ë²¨ ë¦¬ìŠ¤íŠ¸)ë¡œ í´ë°±í•©ë‹ˆë‹¤.
    ìµœì¢… í˜•ì‹(í…ìŠ¤íŠ¸)ì€ LLMì´ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ í”„ë¡¬í”„íŠ¸ì—ì„œ 'ê·¸ëŒ€ë¡œ í¬í•¨'í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
    """
    lines: List[str] = []
    used = 0

    # 1) í›„ë³´ JSON ìš°ì„ 
    if option_candidates and isinstance(option_candidates.get("candidates"), list):
        for c in option_candidates["candidates"]:
            if used >= max_items:
                break
            label = str(c.get("label") or "").strip() or "ì„ íƒì§€"
            reasons = c.get("reasons") or []
            # 2~4ì¤„ ê·¼ê±° ì œí•œ
            reasons = [str(r).strip() for r in reasons if r]
            if len(reasons) > 4:
                reasons = reasons[:4]
            # í•µì‹¬ ë©”íŠ¸ë¦­ì„ ê´„í˜¸ë¡œ 1ì¤„ ìš”ì•½
            metrics = c.get("metrics") or {}
            mkeys = ["revenue","growth_pct","gm","conversion_rate","repeat_rate","aov","inventory_days","return_rate"]
            mparts = []
            for k in mkeys:
                if k in metrics and metrics[k] not in (None, ""):
                    mparts.append(f"{k}: {metrics[k]}")
            metrics_line = f" ({'; '.join(mparts)})" if mparts else ""
            used += 1
            idx = used
            lines.append(f"{idx}. {label}{metrics_line}")
            for r in reasons[:4]:
                lines.append(f"   - {r}")

    # 2) í´ë°±: slots.product_options
    if used == 0 and slots and slots.product_options:
        for i, label in enumerate(slots.product_options, start=1):
            if used >= max_items:
                break
            used += 1
            lines.append(f"{i}. {label}")

    # 3) ê³µí†µ: ë§¨ ëì— '0. ê¸°íƒ€(ì§ì ‘ ì…ë ¥)'
    lines.append("0. ê¸°íƒ€(ì§ì ‘ ì…ë ¥)")
    return "\n".join(lines)

# ===== P1-4: íˆ´ ì˜¤ë¥˜ ìš”ì•½ ë Œë”ëŸ¬ =====
def _render_tool_errors_text(tr: Dict[str, Any]) -> str:
    msgs: List[str] = []
    for key, val in (tr or {}).items():
        if isinstance(val, dict) and val.get("error"):
            tool = key.split("_")[0]
            err = val.get("error")
            if err == "timeout":
                msgs.append(f"{tool} ë„êµ¬ê°€ ì œí•œ ì‹œê°„(12ì´ˆ)ì„ ì´ˆê³¼í•˜ì—¬ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                msgs.append(f"{tool} ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•´ ìƒëµí–ˆìŠµë‹ˆë‹¤.")
    if not msgs:
        return ""
    return "ì°¸ê³ : " + " ".join(msgs)

# ===== Nodes =====
def _merge_slots(state: OrchestratorState, updates: Dict[str, Any]) -> PromotionSlots:
    current = (state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots())
    base = current.model_dump()
    for k, v in updates.items():
        if v not in (None, "", []):
            base[k] = v
    merged = PromotionSlots(**base)
    if state.get("active_task"):
        state["active_task"].slots = merged
    return merged

def slot_extractor_node(state: OrchestratorState):
    logger.info("--- ğŸ” ìŠ¬ë¡¯ ì¶”ì¶œ/ì €ì¥ ë…¸ë“œ ì‹¤í–‰ ---")
    user_message = state.get("user_message", "")
    chat_id = state["chat_id"]

    parser = PydanticOutputParser(pydantic_object=PromotionSlotUpdate)
    prompt_tmpl = textwrap.dedent("""
    ì•„ë˜ í•œêµ­ì–´ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ **í”„ë¡œëª¨ì…˜ ìŠ¬ë¡¯ ê°’**ì„ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.
    - ì¡´ì¬í•˜ëŠ” ê°’ë§Œ ì±„ìš°ê³ , ì—†ìœ¼ë©´ nullë¡œ ë‘ì„¸ìš”.
    - target_typeì€ "brand_target" ë˜ëŠ” "category_target" ì¤‘ í•˜ë‚˜ë¡œë§Œ.
    - ë‚ ì§œ/ê¸°ê°„ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë¬¸ìì—´ë¡œ ìœ ì§€.
    - ì¶œë ¥ì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ì„¸ìš”:
      {format_instructions}

    [ì‚¬ìš©ì ë©”ì‹œì§€]
    {user_message}
    """)
    prompt = ChatPromptTemplate.from_template(
        prompt_tmpl,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        model_kwargs={"response_format": {"type": "json_object"}},
        api_key=settings.GOOGLE_API_KEY
    )
    parsed: PromotionSlotUpdate = (prompt | llm | parser).invoke({"user_message": user_message})

    updates = {k: v for k, v in parsed.model_dump().items() if v not in (None, "", [])}
    if not updates:
        logger.info("ìŠ¬ë¡¯ ì—…ë°ì´íŠ¸ ì—†ìŒ")
        return {}

    try:
        update_state(chat_id, updates)
        logger.info("Mongo ìƒíƒœ ì—…ë°ì´íŠ¸: %s", updates)
    except Exception as e:
        logger.error("Mongo ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: %s", e)

    merged = _merge_slots(state, updates)
    logger.info("State ìŠ¬ë¡¯ ë³‘í•©: %s", merged.model_dump())
    return {"tool_results": {"slot_updates": updates}}

def _planner_router(state: OrchestratorState) -> str:
    instr = state.get("instructions")
    if not instr:
        return "response_generator"  # ê¸°ë³¸ ë¶„ê¸°: ë¹ˆ íˆ´ ì‹¤í–‰ ë°©ì§€

    resp = (instr.response_generator_instruction or "").strip()
    if resp.startswith("[PROMOTION]"):
        return "slot_extractor"

    if instr.tool_calls and len(instr.tool_calls) > 0:
        return "tool_executor"

    return "response_generator"

def planner_node(state: OrchestratorState):
    logger.info("--- ğŸ¤” ê³„íš ìˆ˜ë¦½ ë…¸ë“œ ì‹¤í–‰ ---")

    parser = PydanticOutputParser(pydantic_object=OrchestratorInstruction)
    history_summary = summarize_history(state.get("history", []))
    active_task_dump = state['active_task'].model_dump_json() if state.get('active_task') else 'null'
    schema_sig = state.get("schema_info", "")
    today = today_kr()

    prompt_template = textwrap.dedent("""
    You are the orchestrator for a marketing agent. Decide what to do this turn using ONLY the provided context.
    You MUST output a JSON that strictly follows: {format_instructions}

    ## Route decision (VERY IMPORTANT)
    - Decide the user's intent as one of:
      - Promotion flow (create/continue a promotion)
      - One-off answer (DB facts via t2s, or knowledge snippet)
      - Out-of-scope guidance
    - If and only if it is **promotion flow**, prefix `response_generator_instruction` with "[PROMOTION]".
    - If it is **out-of-scope guidance**, prefix with "[OUT_OF_SCOPE]".
    - Otherwise (one-off answer), no prefix.

    ## Tools (MINIMIZE)
    - ë¨¼ì € ì§ˆë¬¸ìœ¼ë¡œ ëª¨í˜¸ì„±ì„ í•´ì†Œí•˜ì„¸ìš”. **íˆ´ í˜¸ì¶œì€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ** ìµœì†Œ ê°œìˆ˜(ê°€ê¸‰ì  1~2ê°œ)ë¡œ ìš”ì²­í•©ë‹ˆë‹¤.
    - ê²½ëŸ‰â†’ì¤‘ëŸ‰ ìˆœì„œë¡œ ë¶„í•´í•˜ì„¸ìš”: `tavily_search`ë¡œ URL/ê°œìš”ë¥¼ ì–»ì€ ë’¤ **ì •ë§ í•„ìš”í•  ë•Œë§Œ** `scrape_webpages`ë¥¼ ì¼ë¶€(top N) URLì— ì ìš©í•©ë‹ˆë‹¤.
    - ë™ì‹œì— ë¬´ê±°ìš´ íˆ´ì„ ì—¬ëŸ¬ ê°œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”. (ê°€ëŠ¥í•˜ë‹¤ë©´ ìˆœì°¨ ê³„íš)
    - ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ê³¼ í˜•ì‹:
      - DB ì¡°íšŒ: `{{"tool": "t2s", "args": {{"instruction": "SQLë¡œ ë³€í™˜í•  ìì—°ì–´ ì§ˆë¬¸"}}}}`
      - ì›¹ ê²€ìƒ‰: `{{"tool": "tavily_search", "args": {{"query": "ê²€ìƒ‰ì–´", "max_results": 5}}}}`
      - ì›¹ ìŠ¤í¬ë˜í•‘: `{{"tool": "scrape_webpages", "args": {{"urls": ["https://...", ...]}}}}`
      - ë§ˆì¼€íŒ… íŠ¸ë Œë“œ: `{{"tool": "marketing_trend_search", "args": {{"question": "ì§ˆë¬¸"}}}}`
      - ë·°í‹° íŠ¸ë Œë“œ: `{{"tool": "beauty_youtuber_trend_search", "args": {{"question": "ì§ˆë¬¸"}}}}`
    - í”„ë¡œëª¨ì…˜ í”Œë¡œìš°ì¸ ê²½ìš° **ì´ë²ˆ í„´ì—ëŠ” íˆ´ì„ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**

    ## Time normalization
    - Convert relative dates to ABSOLUTE ranges with Asia/Seoul timezone. Today is {today}.

    ## DB schema signature (hint only):
    {schema_sig}

    ## Conversation summary (last turns):
    {history_summary}

    ## Active task snapshot (JSON or null):
    {active_task}

    ## Decision rules
    - Promotion flow: do NOT call tools this turn. Just set `response_generator_instruction` (with [PROMOTION]).
    - One-off answers: set `tool_calls` as needed (MINIMIZED & DECOMPOSED).
    - Out-of-scope: both tools null, and provide short polite guidance with [OUT_OF_SCOPE].
    - Output must be concise, Korean polite style.

    User Message: "{user_message}"
    """)

    prompt = ChatPromptTemplate.from_template(
        template=prompt_template,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        model_kwargs={"response_format": {"type": "json_object"}},
        api_key=settings.GOOGLE_API_KEY
    )

    instructions = (prompt | llm | parser).invoke({
        "user_message": state['user_message'],
        "history_summary": history_summary,
        "active_task": active_task_dump,
        "schema_sig": schema_sig,
        "today": today,
    })

    return {"instructions": instructions}

def action_state_node(state: OrchestratorState):
    logger.info("--- ğŸ“‹ ì•¡ì…˜ ìƒíƒœ í™•ì¸ ë…¸ë“œ ì‹¤í–‰ ---")
    slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else None
    decision = get_action_state(slots=slots)
    logger.info("Action decision: %s", decision)
    return {"tool_results": {"action": decision}}

def _action_router(state: OrchestratorState) -> str:
    tr = state.get("tool_results") or {}
    action = tr.get("action") or {}
    status = action.get("status")
    missing = action.get("missing_slots", [])

    if status == "ask_for_product":
        return "options_generator"

    if status == "ask_for_slots" and any(m in ("brand", "target") for m in missing):
        return "options_generator"

    return "response_generator"

def _build_candidate_t2s_instruction(target_type: str, slots: PromotionSlots) -> str:
    end = datetime.now(ZoneInfo("Asia/Seoul")).date()
    start = end - timedelta(days=60)

    brand_filter_instruction = ""
    if slots and slots.brand:
        brand_filter_instruction = f" ë˜í•œ, ê²°ê³¼ëŠ” ë°˜ë“œì‹œ '{slots.brand}' ë¸Œëœë“œì˜ ì œí’ˆë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."

    if target_type == "brand_target":
        return textwrap.dedent(f"""
        ìµœê·¼ ê¸°ê°„ {start}~{end}ì™€ ì§ì „ ë™ì¼ ê¸°ê°„ì„ ë¹„êµí•˜ì—¬ ë¸Œëœë“œ ë ˆë²¨ í›„ë³´ ëª©ë¡ì„ ì‚°ì¶œí•´ ì£¼ì„¸ìš”.{brand_filter_instruction}
        ë°˜ë“œì‹œ ë‹¤ìŒ ì»¬ëŸ¼ aliasë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
        - brand_name
        - revenue (ìµœê·¼ ê¸°ê°„ ë§¤ì¶œ)
        - growth_pct (ì´ì „ ë™ì¼ê¸°ê°„ ëŒ€ë¹„ ì¦ê°ìœ¨, %)
        - gm (ìµœê·¼ ê¸°ê°„ ì´ì´ìµë¥ , 0~1)
        - conversion_rate
        - repeat_rate
        - aov
        - inventory_days
        - return_rate
        - category_name
        - price_band
        - gender_age
        í–‰ì€ ë¸Œëœë“œë³„ 1í–‰ì…ë‹ˆë‹¤. ìµœê·¼ ê¸°ê°„ ë§¤ì¶œ ìƒìœ„ 100ê°œ ë‚´ì—ì„œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
        """).strip()
    else:
        return textwrap.dedent(f"""
        ìµœê·¼ ê¸°ê°„ {start}~{end}ì™€ ì§ì „ ë™ì¼ ê¸°ê°„ì„ ë¹„êµí•˜ì—¬ ì¹´í…Œê³ ë¦¬/ìƒí’ˆ ë ˆë²¨ í›„ë³´ ëª©ë¡ì„ ì‚°ì¶œí•´ ì£¼ì„¸ìš”.
        ê°€ëŠ¥í•œ ê²½ìš° ë‹¤ìŒ ì»¬ëŸ¼ aliasë¥¼ í¬í•¨í•˜ì„¸ìš”:
        - product_id
        - product_name
        - category_name
        - revenue
        - growth_pct
        - gm
        - conversion_rate
        - repeat_rate
        - aov
        - inventory_days
        - return_rate
        - price_band
        - gender_age
        í–‰ì€ ìƒí’ˆ(ë˜ëŠ” ì¹´í…Œê³ ë¦¬)ë³„ 1í–‰ì…ë‹ˆë‹¤. ìµœê·¼ ê¸°ê°„ ë§¤ì¶œ ìƒìœ„ 200ê°œ ë‚´ì—ì„œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.
        """).strip()

def options_generator_node(state: OrchestratorState):
    logger.info("--- ğŸ§  ì˜µì…˜ ì œì•ˆ ë…¸ë“œ ì‹¤í–‰ ---")
    chat_id = state["chat_id"]
    slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots()
    target_type = slots.target_type or "brand_target"

    t2s_instr = _build_candidate_t2s_instruction(target_type, slots)
    table = run_t2s_agent_with_instruction(state, t2s_instr)
    rows = table["rows"]

    if not rows:
        logger.warning("t2s í›„ë³´ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        update_state(chat_id, {"product_options": []})
        tr = state.get("tool_results") or {}
        tr["option_candidates"] = {"candidates": [], "method": "deterministic_v1", "time_window": "", "constraints": {}}
        return {"tool_results": tr}

    knowledge = get_knowledge_snapshot()
    trending_terms = knowledge.get("trending_terms", [])

    enriched = compute_opportunity_score(rows, trending_terms)
    topk = pick_diverse_top_k(enriched, k=4)

    labels: List[str] = []
    candidates: List[Dict[str, Any]] = []
    for r in topk:
        if target_type == "brand_target":
            cid = f"brand:{r.get('brand_name')}"
            label = str(r.get("brand_name") or "ì•Œ ìˆ˜ ì—†ëŠ” ë¸Œëœë“œ")
            typ = "brand"
        else:
            name = r.get("product_name") or r.get("category_name") or "ì•Œ ìˆ˜ ì—†ëŠ” í•­ëª©"
            cid = f"product:{r.get('product_id') or name}"
            label = str(name)
            typ = "product" if r.get("product_name") else "category"
        labels.append(label)
        candidates.append({
            "id": cid,
            "label": label,
            "type": typ,
            "metrics": {k: r.get(k) for k in (
                "revenue","growth_pct","gm","conversion_rate","repeat_rate","aov","inventory_days","seasonality_score","return_rate"
            ) if k in r},
            "opportunity_score": r.get("opportunity_score"),
            "reasons": r.get("reasons", []),
            "diversity_tags": [x for x in (r.get("category_name"), r.get("price_band"), r.get("gender_age")) if x],
        })

    option_json = {
        "candidates": candidates,
        "method": "deterministic_v1",
        "time_window": "",
        "constraints": {"min_gm": 0.25, "max_return_rate": 0.1},
    }

    try:
        update_state(chat_id, {"product_options": labels})
    except Exception as e:
        logger.error("ì˜µì…˜ ë¼ë²¨ ì €ì¥ ì‹¤íŒ¨: %s", e)

    merged_slots = _merge_slots(state, {"product_options": labels})
    logger.info("ì˜µì…˜ ë¼ë²¨ state ë°˜ì˜: %s", merged_slots.product_options)

    tr = state.get("tool_results") or {}
    tr["option_candidates"] = option_json
    return {"tool_results": tr}

def _parse_knowledge_calls(instr: Optional[Union[str, List[Dict[str, Any]], Dict[str, Any]]]) -> List[Dict[str, Any]]:
    if instr is None:
        return []
    if isinstance(instr, dict):
        return [instr]
    if isinstance(instr, list):
        return [c for c in instr if isinstance(c, dict)]
    if isinstance(instr, str):
        try:
            data = json.loads(instr)
            if isinstance(data, dict):
                return [data]
            if isinstance(data, list):
                return [c for c in data if isinstance(c, dict)]
            return []
        except Exception:
            return []
    return []

def tool_executor_node(state: OrchestratorState):
    logger.info("--- ğŸ”¨ íˆ´ ì‹¤í–‰ ë…¸ë“œ ì‹¤í–‰ ---")
    instructions = state.get("instructions")
    tool_calls = instructions.tool_calls if instructions and instructions.tool_calls else []
    if not tool_calls:
        logger.info("ì‹¤í–‰í•  íˆ´ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {"tool_results": None}

    tool_map = {
        "t2s": lambda args: run_t2s_agent_with_instruction(state, args.get("instruction", "")),
        "tavily_search": lambda args: run_tavily_search(args.get("query", ""), args.get("max_results", 5)),
        "scrape_webpages": lambda args: scrape_webpages(args.get("urls", [])),
        "marketing_trend_search": lambda args: marketing_trend_search(args.get("question", "")),
        "beauty_youtuber_trend_search": lambda args: beauty_youtuber_trend_search(args.get("question", "")),
    }
    # â± ë„êµ¬ë³„ íƒ€ì„ì•„ì›ƒ
    TOOL_TIMEOUTS = {
        "t2s": 60, 
        "scrape_webpages": 60,
        "tavily_search": 60,
        "marketing_trend_search": 60,
        "beauty_youtuber_trend_search": 60,
    }

    tool_results = {}
    MAX_WORKERS = 3
    with ThreadPoolExecutor(max_workers=min(len(tool_calls), MAX_WORKERS)) as executor:
        future_to_meta = {}
        for i, call in enumerate(tool_calls):
            name = call.get("tool")
            args = call.get("args", {})
            if name not in tool_map:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ '{name}' í˜¸ì¶œì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            key = f"{name}_{i}"
            logger.info(f"ğŸ§© {name} ì‹¤í–‰")
            fut = executor.submit(tool_map[name], args)
            future_to_meta[fut] = (key, name, time.time())

        for fut, (key, name, start_ts) in future_to_meta.items():
            timeout = TOOL_TIMEOUTS.get(name, 12)
            try:
                result = fut.result(timeout=timeout)
                took = time.time() - start_ts
                logger.info(f"âœ… '{key}' ì™„ë£Œ | {took:.2f}s")
                tool_results[key] = result
            except TimeoutError:
                logger.error(f"'{key}' íˆ´ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ({timeout}s)")
                tool_results[key] = {"error": "timeout", "message": f"ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤({timeout}ì´ˆ).", "tool": name}
            except Exception as e:
                logger.error(f"'{key}' íˆ´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                tool_results[key] = {"error": "runtime", "message": str(e), "tool": name}

    existing = state.get("tool_results") or {}
    return {"tool_results": {**existing, **tool_results}}

def _should_visualize_router(state: OrchestratorState) -> str:
    tool_results = state.get("tool_results", {})
    for key, value in tool_results.items():
        if key.startswith("t2s") and value and value.get("rows"):
            logger.info("T2S ê²°ê³¼ê°€ ìˆì–´ ì‹œê°í™”ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
            return "visualize"
    logger.info("T2S ê²°ê³¼ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    return "skip_visualize"

def visualizer_caller_node(state: OrchestratorState):
    logger.info("--- ğŸ“Š ì‹œê°í™” ë…¸ë“œ ì‹¤í–‰ ---")
    t2s_result = None
    tool_results = state.get("tool_results", {})
    for key, value in tool_results.items():
        if key.startswith("t2s") and value and value.get("rows"):
            t2s_result = value
            break
    if not t2s_result:
        logger.info("ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {}
    visualizer_app = build_visualize_graph(model="gemini-2.5-flash")
    viz_state = VisualizeState(
        user_question=state.get("user_message"),
        instruction="ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì•„ë˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê³  ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        json_data=json.dumps(t2s_result, ensure_ascii=False)
    )
    viz_response = visualizer_app.invoke(viz_state)
    if viz_response:
        tool_results["visualization"] = {
            "json_graph": viz_response.get("json_graph")       
             }
    return {"tool_results": tool_results}

def _render_table_md(table: Dict[str, Any], max_rows: int = 10) -> str:
    cols = table.get("columns") or []
    rows = (table.get("rows") or [])[:max_rows]

    def esc(x):
        s = "" if x is None else str(x)
        # ë§ˆí¬ë‹¤ìš´ íŒŒì´í”„/ê°œí–‰ ì´ìŠ¤ì¼€ì´í”„
        return s.replace("|", r"\|").replace("\n", " ").replace("\r", " ")

    if not cols:
        # rowsê°€ dictë©´ í‚¤ë¥¼ ì¶”ì¶œ
        if rows and isinstance(rows[0], dict):
            cols = list(rows[0].keys())
        else:
            return ""

    header = "| " + " | ".join(esc(c) for c in cols) + " |"
    sep = "| " + " | ".join("---" for _ in cols) + " |"
    lines = [header, sep]

    for r in rows:
        if isinstance(r, dict):
            vals = [esc(r.get(c, "")) for c in cols]
        else:
            vals = [esc(v) for v in r]
        lines.append("| " + " | ".join(vals) + " |")

    return "\n".join(lines)

def response_generator_node(state: OrchestratorState):
    logger.info("--- ğŸ—£ï¸ ì‘ë‹µ ìƒì„± ë…¸ë“œ (callback streaming via .invoke) ---")
    instructions = state.get("instructions")
    tr = state.get("tool_results") or {}

    instructions_text = (
        instructions.response_generator_instruction
        if instructions and instructions.response_generator_instruction
        else "ì‚¬ìš©ì ìš”ì²­ì— ë§ì¶° ì •ì¤‘í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    )

    # íˆ´ ê²°ê³¼ ìˆ˜ì§‘
    t2s_table = None
    web_search = None
    scraped_pages = None
    marketing_trend_results = None
    youtuber_trend_results = None
    for key, value in tr.items():
        if key.startswith("t2s") and isinstance(value, dict) and "rows" in value:
            t2s_table = value
        elif key.startswith("tavily_search"):
            web_search = value
        elif key.startswith("scrape_webpages"):
            scraped_pages = value
        elif key.startswith("marketing_trend_search"):
            marketing_trend_results = value
        elif key.startswith("beauty_youtuber_trend_search"):
            youtuber_trend_results = value

    action_decision   = tr.get("action")
    knowledge_snippet = tr.get("knowledge") if isinstance(tr.get("knowledge"), str) else None
    option_candidates = tr.get("option_candidates") if isinstance(tr.get("option_candidates"), dict) else None

    # ì„ íƒì§€/ì˜¤ë¥˜ ìš”ì•½/ê¸¸ì´ íŒíŠ¸
    slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots()
    option_list_text  = _render_option_list_text(option_candidates, slots)
    tool_errors_text  = _render_tool_errors_text(tr)
    length_hint       = _compute_length_hint(tr, option_candidates, t2s_table)

    prompt_tmpl = textwrap.dedent("""
    # ê¸¸ì´ ê·œì¹™(í•„ìˆ˜) â†’ {length_hint}
    - short=4~6ë¬¸ì¥, medium=7~12ë¬¸ì¥, long=13~20ë¬¸ì¥

    ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì˜ ìµœì¢… ì‘ë‹µ ìƒì„±ê¸°ì…ë‹ˆë‹¤.
    ì•„ë˜ ì…ë ¥ë§Œì„ ê·¼ê±°ë¡œ **í•œêµ­ì–´ ì¡´ëŒ“ë§**ë¡œ í•œ ë²ˆì— ì™„ì„±ëœ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
    ë‚´ë¶€ ë„êµ¬ëª…/ì‹œìŠ¤í…œ êµ¬í˜„ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

    [ì‘ì„± ìš°ì„ ìˆœìœ„]
    1) action_decision.ask_promptsê°€ ìˆìœ¼ë©´ ê°€ì¥ ë¨¼ì € ê³µì†í•œ í•œ ë¬¸ì¥ ì§ˆë¬¸.
    2) ë‹¤ìŒ ì¤„ì— **option_list_text**ë¥¼ ë³€í˜• ì—†ì´ ê·¸ëŒ€ë¡œ ì¶œë ¥.
    3) ê·¼ê±°/ì„¤ëª…ì€ 2~4ì¤„(ìˆ˜ì¹˜ê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ í‘œê¸°).
    4) (ì£¼ì˜) [TABLE_START]/[TABLE_END] í† í°ì€ **ì„œë²„ê°€ ì‚½ì…**í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    5) ë§ˆì§€ë§‰ ì¤„: ë‹¤ìŒ ë‹¨ê³„ 1ë¬¸ì¥.
    6) tool_errors_textê°€ ìˆìœ¼ë©´ ë§¨ ë í•œ ì¤„ì—ë§Œ ì²¨ë¶€(â€œì°¸ê³ : â€¦â€).

    - instructions_text:
    {instructions_text}

    - action_decision (JSON):
    {action_decision_json}

    - option_list_text (ê·¸ëŒ€ë¡œ ì¶œë ¥):
    {option_list_text}

    - t2s_table (ìš”ì•½ JSON):
    {t2s_table_json}

    - web_search (JSON):
    {web_search_json}

    - scraped_pages (JSON):
    {scraped_pages_json}

    - marketing_trend_results (JSON):
    {marketing_trend_results_json}

    - youtuber_trend_results (JSON):
    {youtuber_trend_results_json}

    - tool_errors_text:
    {tool_errors_text}

    - knowledge_snippet:
    {knowledge_snippet}
    """)

    to_json = lambda x: json.dumps(x, ensure_ascii=False) if x is not None else "null"

    prompt = ChatPromptTemplate.from_template(prompt_tmpl)
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        api_key=settings.GOOGLE_API_KEY,
    )
    chain = prompt | llm

    inputs = {
        "length_hint": length_hint,
        "instructions_text": instructions_text,
        "action_decision_json": to_json(action_decision),
        "option_list_text": option_list_text,
        "t2s_table_json": to_json({
            "columns": (t2s_table or {}).get("columns"),
            "row_count": (t2s_table or {}).get("row_count")
        }),
        "web_search_json": to_json(web_search),
        "scraped_pages_json": to_json(scraped_pages),
        "marketing_trend_results_json": to_json(marketing_trend_results),
        "youtuber_trend_results_json": to_json(youtuber_trend_results),
        "tool_errors_text": tool_errors_text or "",
        "knowledge_snippet": knowledge_snippet or "",
    }

    res = chain.invoke(inputs)
    final_response = getattr(res, "content", None) or str(res) or ""

    table_md = ""
    if t2s_table and t2s_table.get("rows"):
        table_md = _render_table_md(t2s_table, max_rows=10)
    if table_md:
        final_response = f"{final_response}\n\n[TABLE_START]\n{table_md}\n[TABLE_END]"

    logger.info(f"ìµœì¢… ê²°ê³¼(callback stream):\n{final_response}")

    history = state.get("history", [])
    history.append({"role": "user", "content": state.get("user_message", "")})
    history.append({"role": "assistant", "content": final_response})
    return {"history": history, "user_message": "", "output": final_response}

# ===== Graph =====
workflow = StateGraph(OrchestratorState)
workflow.add_node("planner", planner_node)
workflow.add_node("slot_extractor", slot_extractor_node)
workflow.add_node("action_state", action_state_node)
workflow.add_node("options_generator", options_generator_node)
workflow.add_node("tool_executor", tool_executor_node)
workflow.add_node("visualizer", visualizer_caller_node)
workflow.add_node("response_generator", response_generator_node)
workflow.set_entry_point("planner")
workflow.add_conditional_edges(
    "planner",
    _planner_router,
    {
        "slot_extractor": "slot_extractor",
        "tool_executor": "tool_executor",
        "response_generator": "response_generator",
    },
)
workflow.add_edge("slot_extractor", "action_state")
workflow.add_conditional_edges(
    "action_state",
    _action_router,
    {
        "options_generator": "options_generator",
        "response_generator": "response_generator",
    },
)
workflow.add_edge("options_generator", "response_generator")
workflow.add_conditional_edges(
    "tool_executor",
    _should_visualize_router,
    {
        "visualize": "visualizer",
        "skip_visualize": "response_generator"
    }
)
workflow.add_edge("visualizer", "response_generator")
workflow.add_edge("response_generator", END)

orchestrator_app = workflow.compile()
