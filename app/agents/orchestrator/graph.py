from __future__ import annotations

import json
import textwrap
import logging
from typing import List, Optional, Dict, Any, Literal, TypedDict, Union
from concurrent.futures import ThreadPoolExecutor
from datetime import timedelta, date, datetime

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers.pydantic import PydanticOutputParser
from langgraph.graph import StateGraph, END

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic

from app.core.config import settings
from app.database.promotion_slots import update_state
from app.agents.promotion.state import get_action_state
from app.agents.visualizer.graph import build_visualize_graph
from app.agents.visualizer.state import VisualizeState
from .state import *
from .tools import *
from .helpers import *

logger = logging.getLogger(__name__)

# ===== Nodes =====
def _generate_llm_recommendations(state: OrchestratorState, rows: List[Dict[str, Any]], knowledge: Dict[str, Any]) -> List[Dict[str, Any]]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ DB ë°ì´í„°ì™€ ì§€ì‹ ìŠ¤ëƒ…ìƒ·ì„ ê¸°ë°˜ìœ¼ë¡œ 5ê°œ ì¶”ì²œ ìƒì„±"""
    
    slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots()
    
    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¤€ë¹„
    promotion_context = {
        "target_type": getattr(slots, 'target_type', None) or "ë¯¸ì •",
        "focus": getattr(slots, 'focus', None) or "ì—†ìŒ",
        "target": getattr(slots, 'target', None) or "ë¯¸ì •",
        "objective": getattr(slots, 'objective', None) or "ë¯¸ì •", 
        "duration": getattr(slots, 'duration', None) or "ë¯¸ì •",
    }
    
    # ìƒìœ„ 20ê°œ ì •ë„ë§Œ LLMì— ì „ë‹¬ (í† í° ì œí•œ ê³ ë ¤) - None ê°’ ì•ˆì „ ì²˜ë¦¬
    def safe_revenue_key(x):
        revenue = x.get('revenue', 0)
        # Noneì´ë‚˜ NaN ê°’ì„ 0ìœ¼ë¡œ ì²˜ë¦¬
        if revenue is None or (isinstance(revenue, (int, float)) and revenue != revenue):  # NaN ì²´í¬
            return 0
        try:
            return float(revenue)
        except (ValueError, TypeError):
            return 0
    
    top_rows = sorted(rows, key=safe_revenue_key, reverse=True)[:20]
    
    prompt = f"""ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í˜„ì¬ í”„ë¡œëª¨ì…˜ ê¸°íš ê³¼ì •ì—ì„œ ì‚¬ìš©ìì—ê²Œ ì œì‹œí•  ìƒìœ„ 5ê°œ ì¶”ì²œ ì˜µì…˜ì„ ì„ ë³„í•˜ê³  ê°ê°ì˜ ìƒì„¸í•œ ê·¼ê±°ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

**í˜„ì¬ í”„ë¡œëª¨ì…˜ ê¸°íš ìƒí™©:**
- íƒ€ê²Ÿ ìœ í˜•: {promotion_context['target_type']}
- í¬ì»¤ìŠ¤: {promotion_context['focus']}
- íƒ€ê²Ÿ ê³ ê°ì¸µ: {promotion_context['target']}  
- ëª©í‘œ: {promotion_context['objective']}
- ê¸°ê°„: {promotion_context['duration']}

**ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼ (ìƒìœ„ 20ê°œ):**
{json.dumps(top_rows, ensure_ascii=False, indent=2, default=str)}

**ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„:**
- íŠ¸ë Œë”© í‚¤ì›Œë“œ: {knowledge.get('trending_terms', [])}
- ê³„ì ˆì„± ìŠ¤íŒŒì´í¬: {knowledge.get('seasonal_spikes', [])}
- ìˆ˜ì§‘ ì†ŒìŠ¤: {knowledge.get('notes', [])}

**ìš”ì²­ì‚¬í•­:**
1. ìœ„ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ìƒìœ„ 5ê°œ ì¶”ì²œì„ ì„ ë³„í•˜ì„¸ìš”
2. ê° ì¶”ì²œë§ˆë‹¤ ë‹¤ìŒ í˜•íƒœë¡œ ìƒì„¸í•œ ê·¼ê±°ë¥¼ 1-3ê°œ ì œì‹œí•˜ì„¸ìš”:
   - "ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼..."ë¡œ ì‹œì‘í•˜ëŠ” DB ê·¼ê±° (í•´ë‹¹ì‹œ)
   - "ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼..."ë¡œ ì‹œì‘í•˜ëŠ” ì™¸ë¶€ íŠ¸ë Œë“œ ê·¼ê±° (í•´ë‹¹ì‹œ)
   - í˜„ì¬ í”„ë¡œëª¨ì…˜ ëª©í‘œì™€ì˜ ì—°ê´€ì„± (í•´ë‹¹ì‹œ)
3. ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ë§ˆì¼€í„°ê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
4. **íƒ€ì… êµ¬ë¶„**: 
   - ë¸Œëœë“œ í”„ë¡œëª¨ì…˜: "type": "brand"
   - ì¹´í…Œê³ ë¦¬ í”„ë¡œëª¨ì…˜ (ì¹´í…Œê³ ë¦¬ ì„ íƒ ë‹¨ê³„): "type": "category"
   - ì¹´í…Œê³ ë¦¬ í”„ë¡œëª¨ì…˜ (ìƒí’ˆ ì„ íƒ ë‹¨ê³„): "type": "product"

**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´, ì„¤ëª…, ì½”ë“œíœìŠ¤ ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**

{{
  "recommendations": [
    {{
      "rank": 1,
      "name": "ìƒí’ˆ/ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ëª…",
      "type": "{promotion_context['target_type']}",
      "id": "ì›ë³¸ ë°ì´í„°ì˜ ì‹ë³„ì",
      "reasons": [
        "ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼ êµ¬ì²´ì ì¸ ê·¼ê±°1",
        "ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ êµ¬ì²´ì ì¸ ê·¼ê±°2",
        "ì¶”ê°€ ë¹„ì¦ˆë‹ˆìŠ¤ ê·¼ê±°3"
      ],
      "metrics_summary": "ì£¼ìš” ì„±ê³¼ ì§€í‘œ ìš”ì•½"
    }}
  ]
}}"""

    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_core.output_parsers.json import JsonOutputParser
    
    logger.info("ğŸ¤– LLM ê¸°ë°˜ ì¶”ì²œ ìƒì„± ì‹œì‘...")
    logger.info("ğŸ“Š ì…ë ¥ ë°ì´í„°: %dê°œ í–‰, íŠ¸ë Œë”© ìš©ì–´: %s", len(top_rows), knowledge.get('trending_terms', []))
    
    try:
        # í”„ë¡¬í”„íŠ¸ í¬ê¸° ì²´í¬ (ë„ˆë¬´ í¬ë©´ ì¶•ì†Œ)
        if len(prompt) > 50000:  # 50KB ì œí•œ
            logger.warning("âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ (%dì), ë°ì´í„° ì¶•ì†Œ ì¤‘...", len(prompt))
            top_rows = top_rows[:10]  # 20ê°œì—ì„œ 10ê°œë¡œ ì¶•ì†Œ
            prompt = f"""ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í˜„ì¬ í”„ë¡œëª¨ì…˜ ê¸°íš ê³¼ì •ì—ì„œ ì‚¬ìš©ìì—ê²Œ ì œì‹œí•  ìƒìœ„ 5ê°œ ì¶”ì²œ ì˜µì…˜ì„ ì„ ë³„í•˜ê³  ê°ê°ì˜ ìƒì„¸í•œ ê·¼ê±°ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

**í˜„ì¬ í”„ë¡œëª¨ì…˜ ê¸°íš ìƒí™©:**
- íƒ€ê²Ÿ ìœ í˜•: {promotion_context['target_type']}
- í¬ì»¤ìŠ¤: {promotion_context['focus']}
- íƒ€ê²Ÿ ê³ ê°ì¸µ: {promotion_context['target']}  
- ëª©í‘œ: {promotion_context['objective']}
- ê¸°ê°„: {promotion_context['duration']}

**ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼ (ìƒìœ„ 10ê°œ):**
{json.dumps(top_rows, ensure_ascii=False, indent=2, default=str)}

**ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„:**
- íŠ¸ë Œë”© í‚¤ì›Œë“œ: {knowledge.get('trending_terms', [])}
- ê³„ì ˆì„± ìŠ¤íŒŒì´í¬: {knowledge.get('seasonal_spikes', [])}
- ìˆ˜ì§‘ ì†ŒìŠ¤: {knowledge.get('notes', [])}

**ìš”ì²­ì‚¬í•­:**
1. ìœ„ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ìƒìœ„ 5ê°œ ì¶”ì²œì„ ì„ ë³„í•˜ì„¸ìš”
2. ê° ì¶”ì²œë§ˆë‹¤ ë‹¤ìŒ í˜•íƒœë¡œ ìƒì„¸í•œ ê·¼ê±°ë¥¼ 1-3ê°œ ì œì‹œí•˜ì„¸ìš”:
   - "ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼..."ë¡œ ì‹œì‘í•˜ëŠ” DB ê·¼ê±° (í•´ë‹¹ì‹œ)
   - "ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼..."ë¡œ ì‹œì‘í•˜ëŠ” ì™¸ë¶€ íŠ¸ë Œë“œ ê·¼ê±° (í•´ë‹¹ì‹œ)
   - í˜„ì¬ í”„ë¡œëª¨ì…˜ ëª©í‘œì™€ì˜ ì—°ê´€ì„± (í•´ë‹¹ì‹œ)
3. ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ë§ˆì¼€í„°ê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
4. **íƒ€ì… êµ¬ë¶„**: 
   - ë¸Œëœë“œ í”„ë¡œëª¨ì…˜: "type": "brand"
   - ì¹´í…Œê³ ë¦¬ í”„ë¡œëª¨ì…˜ (ì¹´í…Œê³ ë¦¬ ì„ íƒ ë‹¨ê³„): "type": "category"
   - ì¹´í…Œê³ ë¦¬ í”„ë¡œëª¨ì…˜ (ìƒí’ˆ ì„ íƒ ë‹¨ê³„): "type": "product"

**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´, ì„¤ëª…, ì½”ë“œíœìŠ¤ ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**

{{
  "recommendations": [
    {{
      "rank": 1,
      "name": "ìƒí’ˆ/ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ëª…",
      "type": "{promotion_context['target_type']}",
      "id": "ì›ë³¸ ë°ì´í„°ì˜ ì‹ë³„ì",
      "reasons": [
        "ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼ êµ¬ì²´ì ì¸ ê·¼ê±°1",
        "ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ êµ¬ì²´ì ì¸ ê·¼ê±°2",
        "ì¶”ê°€ ë¹„ì¦ˆë‹ˆìŠ¤ ê·¼ê±°3"
      ],
      "metrics_summary": "ì£¼ìš” ì„±ê³¼ ì§€í‘œ ìš”ì•½"
    }}
  ]
}}"""

        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.1,
            model_kwargs={"response_format": {"type": "json_object"}},
            api_key=settings.GOOGLE_API_KEY
        )
        
        logger.info("ğŸ“¤ LLM í˜¸ì¶œ ì¤‘... (í”„ë¡¬í”„íŠ¸ í¬ê¸°: %dì)", len(prompt))
        response = llm.invoke(prompt)
        logger.info("âœ… LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
        
        # JSON íŒŒì‹±
        try:
            result = json.loads(response.content)
            recommendations = result.get("recommendations", [])
            
            logger.info("ğŸ“‹ LLM ì¶”ì²œ ê²°ê³¼:")
            for i, rec in enumerate(recommendations):
                logger.info("  %d. %s (%s)", i+1, rec.get("name"), rec.get("type"))
                logger.info("     ê·¼ê±°: %s", rec.get("reasons", [])[:2])
            
            return recommendations
            
        except json.JSONDecodeError as e:
            logger.error("âŒ LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: %s", e)
            logger.error("ì‘ë‹µ ë‚´ìš©: %s", response.content[:500])
            
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ ì‹œë„
            try:
                content = response.content
                # ```jsonê³¼ ``` ì‚¬ì´ì˜ ë‚´ìš© ì¶”ì¶œ
                if "```json" in content:
                    start = content.find("```json") + 7
                    end = content.find("```", start)
                    if end > start:
                        json_content = content[start:end].strip()
                        result = json.loads(json_content)
                        recommendations = result.get("recommendations", [])
                        logger.info("âœ… JSON ë¸”ë¡ì—ì„œ ì¶”ì¶œ ì„±ê³µ: %dê°œ ì¶”ì²œ", len(recommendations))
                        return recommendations
            except Exception as extract_error:
                logger.error("âŒ JSON ë¸”ë¡ ì¶”ì¶œë„ ì‹¤íŒ¨: %s", extract_error)
            
            return []
            
    except Exception as e:
        logger.error("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: %s", e)
        return []

def _merge_slots(state: OrchestratorState, updates: Dict[str, Any]) -> PromotionSlots:
    current = (state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots())
    base = current.model_dump()
    for k, v in updates.items():
        if v not in (None, "", []):
            # ë¦¬ìŠ¤íŠ¸ íƒ€ì… í•„ë“œë“¤ì€ ê¸°ì¡´ ê°’ê³¼ ë³‘í•©
            if k in ("selected_product", "product_options") and isinstance(v, list):
                existing = base.get(k, [])
                if isinstance(existing, list):
                    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ë³‘í•©
                    base[k] = list(set(existing + v))
                else:
                    base[k] = v
            else:
                base[k] = v
    merged = PromotionSlots(**base)
    if state.get("active_task"):
        state["active_task"].slots = merged
    return merged

def slot_extractor_node(state: OrchestratorState):
    logger.info("--- ğŸ” ìŠ¬ë¡¯ ì¶”ì¶œ/ì €ì¥ ë…¸ë“œ ì‹¤í–‰ ---")
    user_message = state.get("user_message", "")
    chat_id = state["chat_id"]

    parser = PydanticOutputParser(pydantic_object=PromotionSlotUpdate)
    prompt_tmpl = textwrap.dedent("""
    ì•„ë˜ í•œêµ­ì–´ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ **í”„ë¡œëª¨ì…˜ ìŠ¬ë¡¯ ê°’**ì„ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.
    
    **ì¶”ì¶œ ê·œì¹™:**
    - ì¡´ì¬í•˜ëŠ” ê°’ë§Œ ì±„ìš°ê³ , ì—†ìœ¼ë©´ nullë¡œ ë‘ì„¸ìš”.
    - ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ í•„ë“œëŠ” ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
    - target_typeì€ "brand" ë˜ëŠ” "category" ì¤‘ í•˜ë‚˜ë¡œë§Œ.
    - ë‚ ì§œ/ê¸°ê°„ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë¬¸ìì—´ë¡œ ìœ ì§€.
    - focus: ì‚¬ìš©ìê°€ ì„ íƒí•œ ë¸Œëœë“œëª… ë˜ëŠ” ì¹´í…Œê³ ë¦¬ëª… (ì˜ˆ: "ë‚˜ì´í‚¤", "ìŠ¤í¬ì¸ ì›¨ì–´")
    - target: íƒ€ê²Ÿ ê³ ê°ì¸µ - ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ê²½ìš°ì—ë§Œ (ì˜ˆ: "20ëŒ€ ë‚¨ì„±", "ì§ì¥ì¸")
    - selected_product: ì‚¬ìš©ìê°€ ì„ íƒí•œ êµ¬ì²´ì ì¸ ìƒí’ˆëª…ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ìƒí’ˆA", "ìƒí’ˆB"])
    - wants_trend: íŠ¸ë Œë“œ ë°˜ì˜ ì—¬ë¶€ (ì˜ˆ: "ì˜ˆ", "ë„¤", "íŠ¸ë Œë“œ", "ì¢‹ì•„", "í•´ì¤˜" â†’ true, "ì•„ë‹ˆì˜¤", "ì•„ë‹ˆ", "ì—†ì´", "ì•ˆí•´", "ê´œì°®ì•„" â†’ false)
    - objective: í”„ë¡œëª¨ì…˜ ëª©í‘œ - ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ê²½ìš°ì—ë§Œ (ì˜ˆ: "ë§¤ì¶œ ì¦ëŒ€", "ì‹ ê·œ ê³ ê° ìœ ì…")
    
    **ê¸ˆì§€ì‚¬í•­:**
    - budget, cost, ì˜ˆì‚° ë“±ì€ ì¶”ì¶œí•˜ì§€ ë§ˆì„¸ìš” (ìŠ¬ë¡¯ì— ì—†ëŠ” í•„ë“œì„)
    - ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ì™€ ìƒí’ˆëª…ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”. ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ëŠ” focus í•„ë“œì—, êµ¬ì²´ì ì¸ ìƒí’ˆì€ selected_productì— ë„£ìœ¼ì„¸ìš”.
    
    **CRITICAL: ì¶œë ¥ì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.**
    
    JSON ìŠ¤í‚¤ë§ˆ:
    {format_instructions}

    [ì‚¬ìš©ì ë©”ì‹œì§€]
    {user_message}
    """)
    prompt = ChatPromptTemplate.from_template(
        prompt_tmpl,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        model_kwargs={"response_format": {"type": "json_object"}},
        api_key=settings.GOOGLE_API_KEY
    )
    parsed: PromotionSlotUpdate = (prompt | llm | parser).invoke({"user_message": user_message})

    updates = {k: v for k, v in parsed.model_dump().items() if v not in (None, "", [])}
    
    # ì´ë¯¸ ì„¤ì •ëœ ìŠ¬ë¡¯ì€ ë³€ê²½í•˜ì§€ ì•ŠìŒ (ê¸°ì¡´ ê°’ ë³´ì¡´)
    current_slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots()
    
    # ì´ë¯¸ ì±„ì›Œì§„ í•„ë“œë“¤ì€ ì—…ë°ì´íŠ¸ì—ì„œ ì œê±°
    preserved_fields = []
    for field in ["target_type", "focus", "duration", "selected_product", "wants_trend"]:
        current_value = getattr(current_slots, field, None)
        if current_value not in (None, "", []) and field in updates:
            preserved_fields.append(field)
            logger.info("%sì´(ê°€) ì´ë¯¸ ì„¤ì •ë¨ (%s), ë³€ê²½ ë°©ì§€", field, current_value)
            del updates[field]
    
    if preserved_fields:
        logger.info("ë³´ì¡´ëœ í•„ë“œë“¤: %s", preserved_fields)
    
    if not updates:
        logger.info("ìŠ¬ë¡¯ ì—…ë°ì´íŠ¸ ì—†ìŒ")
        return {}

    try:
        update_state(chat_id, updates)
        logger.info("Mongo ìƒíƒœ ì—…ë°ì´íŠ¸: %s", updates)
    except Exception as e:
        logger.error("Mongo ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: %s", e)

    merged = _merge_slots(state, updates)
    logger.info("State ìŠ¬ë¡¯ ë³‘í•©: %s", merged.model_dump())
    return {"tool_results": {"slot_updates": updates}}

def _planner_router(state: OrchestratorState) -> str:
    instr = state.get("instructions")
    if not instr:
        logger.warning("âš ï¸ instructionsê°€ Noneì…ë‹ˆë‹¤. planner_nodeì—ì„œ ì—ëŸ¬ ë°œìƒí–ˆì„ ê°€ëŠ¥ì„± ë†’ìŒ")
        logger.warning("ìƒíƒœ ì •ë³´: user_message='%s'", state.get('user_message', 'N/A')[:100])
        return "response_generator"
        
    resp = (instr.response_generator_instruction or "").strip()
    logger.info("ë¼ìš°íŒ… ê²°ì • ì¤‘: response_instruction='%s'", resp[:50])
    
    if resp.startswith("[PROMOTION]"):
        logger.info("â†’ í”„ë¡œëª¨ì…˜ í”Œë¡œìš°: slot_extractor")
        return "slot_extractor"
    

    if instr.tool_calls and len(instr.tool_calls) > 0:
        logger.info("â†’ íˆ´ í˜¸ì¶œ í•„ìš”: tool_executor (%dê°œ íˆ´)", len(instr.tool_calls))
        return "tool_executor"
        
    logger.info("â†’ ì‘ë‹µ ìƒì„±: response_generator")
    return "response_generator"

def trend_planner_node(state: OrchestratorState):
    """íŠ¸ë Œë“œ ë°˜ì˜ì„ ìœ„í•œ ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ ê³„íš"""
    logger.info("--- ğŸŒŸ íŠ¸ë Œë“œ ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½ ë…¸ë“œ ì‹¤í–‰ ---")
    
    slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots()
    
    # í”„ë¡œëª¨ì…˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¸ë Œë“œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    search_queries = []
    
    # ê¸°ë³¸ ê²€ìƒ‰ì–´ êµ¬ì„±
    base_query = f"{slots.focus or ''} {slots.target or ''} í”„ë¡œëª¨ì…˜ ë§ˆì¼€íŒ…"
    search_queries.append(base_query.strip())
    
    # íƒ€ê²Ÿë³„ íŠ¸ë Œë“œ ê²€ìƒ‰
    if slots.target:
        search_queries.append(f"{slots.target} íŠ¸ë Œë“œ ìœ í–‰ì–´ ë°ˆ")
    
    # ìƒí’ˆë³„ íŠ¸ë Œë“œ ê²€ìƒ‰  
    if slots.selected_product:
        for product in slots.selected_product[:2]:  # ìµœëŒ€ 2ê°œ ìƒí’ˆë§Œ
            search_queries.append(f"{product} ë§ˆì¼€íŒ… íŠ¸ë Œë“œ")
    
    # ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ íˆ´ í˜¸ì¶œ êµ¬ì„±
    tool_calls = [
        {"tool": "tavily_search", "args": {"query": search_queries[0], "max_results": 5}},
        {"tool": "marketing_trend_search", "args": {"question": f"{slots.focus} {slots.target} í”„ë¡œëª¨ì…˜ ê´€ë ¨ ìµœì‹  íŠ¸ë Œë“œ"}},
    ]
    
    # ë·°í‹° ê´€ë ¨ì´ë©´ ìœ íŠœë²„ íŠ¸ë Œë“œë„ ì¶”ê°€
    if any(keyword in (slots.focus or "").lower() for keyword in ["í™”ì¥í’ˆ", "ë·°í‹°", "ì½”ìŠ¤ë©”í‹±", "ìŠ¤í‚¨ì¼€ì–´"]):
        tool_calls.append({
            "tool": "beauty_youtuber_trend_search", 
            "args": {"question": f"{slots.focus} ë·°í‹° íŠ¸ë Œë“œ"}
        })
    
    return {
        "instructions": OrchestratorInstruction(
            tool_calls=tool_calls,
            response_generator_instruction="íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í”„ë¡œëª¨ì…˜ì— ì ìš©í•  ì¤€ë¹„ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        )
    }

def planner_node(state: OrchestratorState):
    logger.info("--- ğŸ¤” ê³„íš ìˆ˜ë¦½ ë…¸ë“œ ì‹¤í–‰ ---")
    logger.info("ì…ë ¥ ìƒíƒœ: user_message='%s', active_task=%s", 
                state.get('user_message', 'N/A')[:100], 
                bool(state.get('active_task')))
    
    try:
        # íŠ¸ë Œë“œ ë°˜ì˜ ìƒíƒœì¸ì§€ í™•ì¸
        tr = state.get("tool_results") or {}
        action = tr.get("action") or {}
        if action.get("status") == "apply_trends":
            logger.info("íŠ¸ë Œë“œ ë°˜ì˜ ìƒíƒœ ê°ì§€, trend_planner_nodeë¡œ ì „í™˜")
            return trend_planner_node(state)

        parser = PydanticOutputParser(pydantic_object=OrchestratorInstruction)
        history_summary = summarize_history(state.get("history", []))
        active_task_dump = state['active_task'].model_dump_json() if state.get('active_task') else 'null'
        schema_sig = state.get("schema_info", "")
        today = today_kr()

        prompt_template = textwrap.dedent("""
    You are the orchestrator for a marketing agent. Decide what to do this turn using ONLY the provided context.
    
    **CRITICAL: You MUST output ONLY valid JSON. No explanations, no markdown, no code blocks - just pure JSON.**
    
    You MUST output a JSON that strictly follows: {format_instructions}

    ## Route decision (VERY IMPORTANT)
    - Decide the user's intent as one of:
      - Promotion flow (create/continue a promotion)
      - One-off answer (DB facts via t2s, or knowledge snippet)
      - Out-of-scope guidance
    - If and only if it is **promotion flow**, prefix `response_generator_instruction` with "[PROMOTION]".
    - If it is **out-of-scope guidance**, prefix with "[OUT_OF_SCOPE]".
    - Otherwise (one-off answer), no prefix.

    ## Tools
    - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ ëª¨ë“  ë„êµ¬ë¥¼ **tool_calls JSON ë°°ì—´**ì— ë‹´ì•„ ìš”ì²­í•˜ì„¸ìš”.
    - í•„ìš”í•˜ë‹¤ë©´ **ì—¬ëŸ¬ ê°œì˜ ë„êµ¬ë¥¼ í•˜ë‚˜ì˜ ë°°ì—´ì— ë™ì‹œì— ìš”ì²­**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ê° ë„êµ¬ ê°ì²´ëŠ” `{{"tool": "ë„êµ¬ëª…", "args": {{"íŒŒë¼ë¯¸í„°ëª…": "ê°’"}}}}` í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
    - ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ê³¼ í˜•ì‹:
      - DB ì¡°íšŒ: `{{"tool": "t2s", "args": {{"instruction": "SQLë¡œ ë³€í™˜í•  ìì—°ì–´ ì§ˆë¬¸", "output_type": "export|visualize|table"}}}}`
        - output_type ì„ íƒ ê°€ì´ë“œë¼ì¸:
          * "export": ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•˜ëŠ” ê²½ìš° (ì˜ˆì‹œ: "í´ë¦­ìœ¨ì´ ê°ì†Œ ì¤‘ì¸ ìœ ì € ID ëª©ë¡", "ì´ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•´ì¤˜", "ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì‹¶ì–´")
          * "visualize": ë°ì´í„° ì‹œê°í™”ê°€ í•„ìš”í•œ ê²½ìš° (ì˜ˆì‹œ: "ë¹„êµ"ë¥¼ í•´ì•¼í•˜ëŠ” ì§ˆë¬¸, "ìƒìœ„ 10ê°œ ë¸Œëœë“œ ì•Œë ¤ì¤˜", "ì¶”ì„¸"ì— ëŒ€í•œ ì§ˆë¬¸, "ì‹œê°í™”í•´ì„œ ë³´ì—¬ì¤˜", "ì°¨íŠ¸ë¡œ ë¶„ì„í•´ì¤˜", "ê·¸ë˜í”„ë¡œ ë¹„êµí•´ì¤˜")
          * "table": ë‹¨ìˆœ íŒ©íŠ¸ í™•ì¸ì´ë‚˜ í‘œ í˜•íƒœë¡œ ë³´ê¸° ì›í•˜ëŠ” ê²½ìš° (ì˜ˆ: "ì‘ë…„ ë§¤ì¶œì´ ì–¼ë§ˆì˜€ì§€?", "ë°ì´í„°ë¥¼ í‘œë¡œ ë³´ì—¬ì¤˜")
      - ì›¹ ê²€ìƒ‰: `{{"tool": "tavily_search", "args": {{"query": "ê²€ìƒ‰ì–´", "max_results": 5}}}}`
      - ì›¹ ìŠ¤í¬ë˜í•‘: `{{"tool": "scrape_webpages", "args": {{"urls": ["https://...", ...]}}}}`
      - ë§ˆì¼€íŒ… íŠ¸ë Œë“œ: `{{"tool": "marketing_trend_search", "args": {{"question": "ì§ˆë¬¸"}}}}`
      - ë·°í‹° íŠ¸ë Œë“œ: `{{"tool": "beauty_youtuber_trend_search", "args": {{"question": "ì§ˆë¬¸"}}}}`
    - **íŠ¸ë Œë“œ ë°˜ì˜ í”„ë¡œëª¨ì…˜**: ë‹¤ìŒ ê²½ìš°ì— ë§ˆì¼€íŒ… íŠ¸ë Œë“œ ìˆ˜ì§‘ íˆ´ë“¤ì„ í˜¸ì¶œí•˜ì„¸ìš”:
      * wants_trend=trueì´ê³  í•„ìš” ìŠ¬ë¡¯ì´ ëª¨ë‘ ì±„ì›Œì§„ ê²½ìš° 
      * ë˜ëŠ” ì´ì „ ë©”ì‹œì§€ê°€ íŠ¸ë Œë“œ ì§ˆë¬¸ì´ê³  í˜„ì¬ ì‚¬ìš©ìê°€ ê¸ì •ì ìœ¼ë¡œ ì‘ë‹µí•œ ê²½ìš° (ì˜ˆ: "ì˜ˆ", "ë„¤", "ì‘", "ì¢‹ì•„", "í•´ì¤˜" ë“±)
    - ë„êµ¬ ì‚¬ìš©ì´ í•„ìš” ì—†ìœ¼ë©´ `tool_calls` í•„ë“œë¥¼ nullë¡œ ë‘ì„¸ìš”.

    ## Time normalization
    - Convert relative dates to ABSOLUTE ranges with Asia/Seoul timezone. Today is {today}.

    ## DB schema signature (hint only):
    {schema_sig}

    ## Conversation summary (last turns):
    {history_summary}

    ## Active task snapshot (JSON or null):
    {active_task}

    ## Decision rules
    - Promotion flow: do NOT call tools this turn. Just set `response_generator_instruction` (with [PROMOTION]).
    - **EXCEPTION 1**: If active_task shows promotion is ready for trend application (wants_trend=true, all slots filled), then call trend tools instead of setting [PROMOTION].
    - **EXCEPTION 2**: If user is responding positively to a trend question (check conversation history for recent trend question + current positive response), then call trend tools even if wants_trend is still None.
    - One-off answers: set `tool_calls` as needed.
    - Out-of-scope: both tools null, and provide short polite guidance with [OUT_OF_SCOPE].
    - Output must be concise, Korean polite style.
    
    ## t2s output_type ì„ íƒ ì˜ˆì‹œ
    - "export" ì„ íƒ ì‹œë‚˜ë¦¬ì˜¤:
      * "ìœ ì € ID ëª©ë¡ì„ ì—‘ì…€ë¡œ ë‚´ë ¤ì¤˜" â†’ output_type: "export"
      * "ì´ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•´ì¤˜" â†’ output_type: "export"  
      * "ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì‹¶ì–´" â†’ output_type: "export"
      * "ì „ì²´ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ë°›ê³  ì‹¶ì–´" â†’ output_type: "export"
    - "visualize" ì„ íƒ ì‹œë‚˜ë¦¬ì˜¤:
      * "ì¶”ì„¸ë¥¼ ê·¸ë˜í”„ë¡œ ë³´ì—¬ì¤˜" â†’ output_type: "visualize"
      * "ì‹œê°í™”í•´ì„œ ë³´ì—¬ì¤˜" â†’ output_type: "visualize"
      * "ì°¨íŠ¸ë¡œ ë¶„ì„í•´ì¤˜" â†’ output_type: "visualize"
      * "ê·¸ë˜í”„ë¡œ ë¹„êµí•´ì¤˜" â†’ output_type: "visualize"
      * "íŠ¸ë Œë“œë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤˜" â†’ output_type: "visualize"
    - "table" ì„ íƒ ì‹œë‚˜ë¦¬ì˜¤:
      * "ì‘ë…„ ë§¤ì¶œì´ ì–¼ë§ˆì˜€ì§€?" â†’ output_type: "table"
      * "ìƒìœ„ 10ê°œ ë¸Œëœë“œ ì•Œë ¤ì¤˜" â†’ output_type: "table"
      * "ë°ì´í„°ë¥¼ í‘œë¡œ ë³´ì—¬ì¤˜" â†’ output_type: "table"
      * "ë§¤ì¶œ ìˆœìœ„ë¥¼ ì•Œë ¤ì¤˜" â†’ output_type: "table"
      * "ì–´ë–¤ ë¸Œëœë“œê°€ ì œì¼ ì˜ íŒ”ë ¸ì–´?" â†’ output_type: "table"

    User Message: "{user_message}"
    """)

        prompt = ChatPromptTemplate.from_template(
            template=prompt_template,
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0,
            model_kwargs={"response_format": {"type": "json_object"}},
            api_key=settings.GOOGLE_API_KEY
        )

        # llm = ChatAnthropic(
        #     model="claude-sonnet-4-20250514", 
        #     temperature=0.1,
        #     max_tokens=8192,  # ë„‰ë„‰í•œ í† í° ì œí•œ ì„¤ì •
        #     api_key=settings.ANTHROPIC_API_KEY
        # )

        logger.info("LLM í˜¸ì¶œ ì¤‘...")
        try:
            instructions = (prompt | llm | parser).invoke({
                "user_message": state['user_message'],
                "history_summary": history_summary,
                "active_task": active_task_dump,
                "schema_sig": schema_sig,
                "today": today,
            })
        except Exception as parse_error:
            logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨, ì¬ì‹œë„: %s", parse_error)
            # ì¬ì‹œë„ (ë” ê°•í•œ í”„ë¡¬í”„íŠ¸ë¡œ)
            retry_prompt = prompt_template + "\n\n**REMINDER: Output must be valid JSON only!**"
            retry_template = ChatPromptTemplate.from_template(
                template=retry_prompt,
                partial_variables={"format_instructions": parser.get_format_instructions()}
            )
            instructions = (retry_template | llm | parser).invoke({
                "user_message": state['user_message'],
                "history_summary": history_summary,
                "active_task": active_task_dump,
                "schema_sig": schema_sig,
                "today": today,
            })

        logger.info("âœ… ê³„íš ìˆ˜ë¦½ ì„±ê³µ: tool_calls=%s, response_instruction=%s", 
                   len(instructions.tool_calls or []), 
                   instructions.response_generator_instruction[:100] if instructions.response_generator_instruction else 'N/A')
        return {"instructions": instructions}
        
    except Exception as e:
        logger.error("âŒ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: %s", e, exc_info=True)
        logger.error("ì—ëŸ¬ íƒ€ì…: %s", type(e).__name__)
        logger.error("ì‚¬ìš©ì ë©”ì‹œì§€: %s", state.get('user_message', 'N/A'))
        logger.error("íˆìŠ¤í† ë¦¬ ê¸¸ì´: %d", len(state.get('history', [])))
        
        # í´ë°± instructions ìƒì„±
        fallback_instructions = OrchestratorInstruction(
            tool_calls=None,
            response_generator_instruction="ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        )
        logger.info("í´ë°± instructions ìƒì„± ì™„ë£Œ")
        return {"instructions": fallback_instructions}

def action_state_node(state: OrchestratorState):
    logger.info("--- ğŸ“‹ ì•¡ì…˜ ìƒíƒœ í™•ì¸ ë…¸ë“œ ì‹¤í–‰ ---")
    active_task = state.get("active_task")
    slots = active_task.slots if active_task and active_task.slots else None
    logger.info("ì…ë ¥: active_task=%s, slots=%s", bool(active_task), slots.model_dump() if slots else None)
    
    decision = get_action_state(slots=slots)
    logger.info("âœ… Action decision: %s", decision)
    return {"tool_results": {"action": decision}}

def _action_router(state: OrchestratorState) -> str:
    """
    action_state ê²°ê³¼ë¡œ ë‹¤ìŒ ë…¸ë“œ ê²°ì •:
    - brand/targetì„ ë¬»ê±°ë‚˜, íŠ¹ì • productë¥¼ ë¬¼ì–´ì•¼ í•˜ëŠ” ìƒí™©ì´ë©´ options_generatorë¡œ ë¶„ê¸°
    - íŠ¸ë Œë“œ ë°˜ì˜ì´ í•„ìš”í•˜ë©´ tool_executorë¡œ ë¶„ê¸°
    - ê·¸ ì™¸ (objective/duration ì§ˆë¬¸, ìµœì¢… í™•ì¸ ë“±)ëŠ” response_generatorë¡œ ë¶„ê¸°
    """
    tr = state.get("tool_results") or {}
    action = tr.get("action") or {}
    status = action.get("status")
    missing = action.get("missing_slots", [])

    # íŠ¸ë Œë“œ ë°˜ì˜ ìƒíƒœì¼ ë•Œ ì™¸ë¶€ ë°ì´í„° íˆ´ í˜¸ì¶œ
    if status == "apply_trends":
        return "tool_executor"
    
    # ìµœì¢… ê¸°íšì„œ ìƒì„± ìƒíƒœ
    if status == "create_final_plan":
        return "response_generator"

    # 'ask_for_product' ìƒíƒœì¼ ë•Œ options_generatorë¥¼ í˜¸ì¶œí•˜ë„ë¡ ëª…ì‹œ
    if status == "ask_for_product":
        return "options_generator"
    
    # ê¸°ì¡´ ë¡œì§: focusë‚˜ targetì„ ë¬¼ì–´ì•¼ í•  ë•Œë„ options_generator í˜¸ì¶œ
    if status == "ask_for_slots" and any(m in ("focus", "target") for m in missing):
        return "options_generator"
        
    return "response_generator"
    
def _build_candidate_t2s_instruction(target_type: str, slots: PromotionSlots) -> str:
    end = datetime.now(ZoneInfo("Asia/Seoul")).date()
    start = end - timedelta(days=30)  # 30ì¼ë¡œ ë‹¨ì¶•
    
    # target ì¡°ê±´ ì¶”ê°€
    target_filter = ""
    if slots and slots.target:
        target_filter = f" '{slots.target}' íƒ€ê²Ÿ ê³ ê°ì¸µì´ ì£¼ë¡œ êµ¬ë§¤í•˜ëŠ”"
    
    if target_type == "brand":
        if slots and slots.focus:
            # ë¸Œëœë“œê°€ ì„ íƒëœ ê²½ìš° í•´ë‹¹ ë¸Œëœë“œì˜ ìƒí’ˆ ì˜µì…˜
            return f"'{slots.focus}' ë¸Œëœë“œì˜{target_filter} ìµœê·¼ 30ì¼ ë§¤ì¶œ ìƒìœ„ 20ê°œ ìƒí’ˆì„ product_name, revenue, growth_pct ì»¬ëŸ¼ìœ¼ë¡œ ì¡°íšŒí•´ ì£¼ì„¸ìš”."
        else:
            # ë¸Œëœë“œ ì„ íƒ ë‹¨ê³„
            return f"{target_filter} ìµœê·¼ 30ì¼ ë§¤ì¶œ ìƒìœ„ 15ê°œ ë¸Œëœë“œë¥¼ brand_name, revenue, growth_pct ì»¬ëŸ¼ìœ¼ë¡œ ì¡°íšŒí•´ ì£¼ì„¸ìš”."
    else:
        if slots and slots.focus:
            # ì¹´í…Œê³ ë¦¬ê°€ ì„ íƒëœ ê²½ìš° í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ìƒí’ˆ ì˜µì…˜
            return f"'{slots.focus}' ì¹´í…Œê³ ë¦¬ì˜{target_filter} ìµœê·¼ 30ì¼ ë§¤ì¶œ ìƒìœ„ 20ê°œ ìƒí’ˆì„ product_name, brand_name, revenue, growth_pct ì»¬ëŸ¼ìœ¼ë¡œ ì¡°íšŒí•´ ì£¼ì„¸ìš”."
        else:
            # ì¹´í…Œê³ ë¦¬ ì„ íƒ ë‹¨ê³„
            return f"{target_filter} ìµœê·¼ 30ì¼ ë§¤ì¶œ ìƒìœ„ 15ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ category_name, revenue, growth_pct ì»¬ëŸ¼ìœ¼ë¡œ ì¡°íšŒí•´ ì£¼ì„¸ìš”."

def options_generator_node(state: OrchestratorState):
    logger.info("--- ğŸ§  ì˜µì…˜ ì œì•ˆ ë…¸ë“œ ì‹¤í–‰ ì‹œì‘ ---")
    logger.info("ğŸ“Š ì…ë ¥ ìƒíƒœ ì •ë³´:")
    logger.info("  - chat_id: %s", state.get("chat_id"))
    logger.info("  - active_task ì¡´ì¬: %s", bool(state.get("active_task")))
    
    chat_id = state["chat_id"]
    slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots()
    target_type = slots.target_type or "brand"
    
    logger.info("ğŸ¯ íƒ€ê²Ÿ íƒ€ì…: %s", target_type)
    logger.info("ğŸ“‹ ìŠ¬ë¡¯ ì •ë³´: %s", slots)

    logger.info("ğŸ”§ T2S ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ìƒì„± ì¤‘...")
    t2s_instr = _build_candidate_t2s_instruction(target_type, slots)
    logger.info("ğŸ“ ìƒì„±ëœ T2S ì¸ìŠ¤íŠ¸ëŸ­ì…˜: %s", t2s_instr[:200] + "..." if len(t2s_instr) > 200 else t2s_instr)
    
    logger.info("ğŸš€ T2S ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...")
    table = run_t2s_agent_with_instruction(state, t2s_instr, "visualize")  # ì˜µì…˜ ìƒì„±ì€ í•­ìƒ ì‹œê°í™” í¬í•¨
    rows = table["rows"]
    
    logger.info("ğŸ“Š T2S ê²°ê³¼ ë¶„ì„:")
    logger.info("  - ì „ì²´ í–‰ ìˆ˜: %d", len(rows))
    logger.info("  - ì»¬ëŸ¼ ì •ë³´: %s", list(table.get("columns", [])))
    
    if rows:
        logger.info("ğŸ“‹ ì²« ë²ˆì§¸ í–‰ ìƒ˜í”Œ: %s", {k: v for k, v in rows[0].items() if k in ['brand_name', 'product_name', 'category_name', 'revenue', 'growth_pct']})

    if not rows:
        logger.warning("âŒ T2S í›„ë³´ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        logger.info("ğŸ”„ ë¹ˆ ê²°ê³¼ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘...")
        update_state(chat_id, {"product_options": []})
        tr = state.get("tool_results") or {}
        tr["option_candidates"] = {"candidates": [], "method": "deterministic_v1", "time_window": "", "constraints": {}}
        logger.info("âœ… ë¹ˆ ì˜µì…˜ í›„ë³´ ë°˜í™˜ ì™„ë£Œ")
        return {"tool_results": tr}

    logger.info("ğŸ” ì§€ì‹ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ ì¤‘...")
    knowledge = get_knowledge_snapshot()
    trending_terms = knowledge.get("trending_terms", [])
    
    logger.info("ğŸ“ˆ íŠ¸ë Œë”© ìš©ì–´ ë¶„ì„:")
    logger.info("  - íŠ¸ë Œë”© ìš©ì–´ ìˆ˜: %d", len(trending_terms))
    logger.info("  - íŠ¸ë Œë”© ìš©ì–´ ëª©ë¡: %s", trending_terms)
    logger.info("  - ê³„ì ˆì„± ìŠ¤íŒŒì´í¬: %s", knowledge.get("seasonal_spikes", []))
    logger.info("  - ìˆ˜ì§‘ ë…¸íŠ¸: %s", knowledge.get("notes", []))

    logger.info("ğŸ¤– LLM ê¸°ë°˜ ì¶”ì²œ ìƒì„± ì¤‘...")
    llm_recommendations = _generate_llm_recommendations(state, rows, knowledge)
    
    if not llm_recommendations:
        logger.warning("âŒ LLM ì¶”ì²œ ìƒì„± ì‹¤íŒ¨ - ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±")
        # í´ë°±: ë‹¨ìˆœ ì ìˆ˜ ê¸°ë°˜ ì„ íƒ
        enriched = compute_opportunity_score(rows, trending_terms)
        # ë‹¤ì–‘ì„± ì œì•½ ì—†ì´ ìƒìœ„ ì ìˆ˜ ìˆœìœ¼ë¡œ ì„ íƒ
        topk = sorted(enriched, key=lambda x: x.get("opportunity_score", 0), reverse=True)[:5]
        
        labels = []
        candidates = []
        for i, r in enumerate(topk):
            if target_type == "brand":
                cid = f"brand:{r.get('brand_name')}"
                label = str(r.get("brand_name") or "ì•Œ ìˆ˜ ì—†ëŠ” ë¸Œëœë“œ")
                typ = "brand"
            else:
                # ì¹´í…Œê³ ë¦¬ íƒ€ì…ì¼ ë•ŒëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œ
                if r.get("category_name") and not slots.focus:
                    # ì¹´í…Œê³ ë¦¬ ì„ íƒ ë‹¨ê³„
                    name = r.get("category_name")
                    cid = f"category:{name}"
                    label = str(name)
                    typ = "category"
                else:
                    # íŠ¹ì • ì¹´í…Œê³ ë¦¬ê°€ ì„ íƒëœ ê²½ìš° ìƒí’ˆ ì¶”ì²œ
                    name = r.get("product_name") or r.get("category_name") or "ì•Œ ìˆ˜ ì—†ëŠ” í•­ëª©"
                    cid = f"product:{r.get('product_id') or name}"
                    label = str(name)
                    typ = "product" if r.get("product_name") else "category"
            
            labels.append(label)
            candidates.append({
                "id": cid,
                "label": label,
                "type": typ,
                "metrics": {k: r.get(k) for k in ("revenue","growth_pct","gm") if k in r},
                "opportunity_score": r.get("opportunity_score"),
                "reasons": r.get("reasons", []),
                "score": r.get("opportunity_score"),
            })
            
            logger.info("  %dë²ˆ í´ë°± ì¶”ì²œ: %s (%s)", i+1, label, typ)
    else:
        logger.info("âœ… LLM ì¶”ì²œ ìƒì„± ì„±ê³µ - %dê°œ ì¶”ì²œ", len(llm_recommendations))
        
        labels = []
        candidates = []
        
        for i, rec in enumerate(llm_recommendations[:5]):  # ìµœëŒ€ 5ê°œ
            # LLM ì¶”ì²œì„ í‘œì¤€ í›„ë³´ í˜•íƒœë¡œ ë³€í™˜
            name = rec.get("name", f"ì¶”ì²œ{i+1}")
            typ = rec.get("type", "product")
            
            # ì›ë³¸ ë°ì´í„°ì—ì„œ í•´ë‹¹ í•­ëª© ì°¾ê¸° (ì¸ë±ìŠ¤ ê¸°ë°˜ ë§¤ì¹­ìœ¼ë¡œ ìµœì í™”)
            original_row = None
            # ì²« ë²ˆì§¸ë¡œ ë§¤ì¹­ë˜ëŠ” í•­ëª© ì‚¬ìš© (ì´ë¯¸ ì •ë ¬ëœ ìƒìœ„ ê²°ê³¼ì—ì„œ ì„ íƒí–ˆìœ¼ë¯€ë¡œ)
            if i < len(rows):
                original_row = rows[i]
            
            if target_type == "brand":
                cid = f"brand:{name}"
            else:
                # ì¹´í…Œê³ ë¦¬ íƒ€ì…ì¼ ë•ŒëŠ” ì¹´í…Œê³ ë¦¬/ìƒí’ˆ êµ¬ë¶„
                if not slots.focus and rec.get("type") == "category":
                    cid = f"category:{name}"
                else:
                    cid = f"product:{rec.get('id', name)}"
            
            labels.append(name)
            
            candidate = {
                "id": cid,
                "label": name,
                "type": typ,
                "llm_reasons": rec.get("reasons", []),  # LLMì´ ìƒì„±í•œ ìƒì„¸ ì„¤ëª…
                "metrics_summary": rec.get("metrics_summary", ""),
                "rank": rec.get("rank", i+1),
                "metrics": {k: original_row.get(k) for k in ("revenue","growth_pct","gm") if original_row and k in original_row} if original_row else {},
            }
            
            candidates.append(candidate)
            
            logger.info("  %dë²ˆ LLM ì¶”ì²œ: %s (%s)", i+1, name, typ)
            logger.info("    - ê·¼ê±° ê°œìˆ˜: %d", len(rec.get("reasons", [])))
            logger.info("    - ë©”íŠ¸ë¦­ ìš”ì•½: %s", rec.get("metrics_summary", "ì—†ìŒ")[:100])

    option_json = {
        "candidates": candidates,
        "method": "simplified_v2",
        "time_window": "30days",
        "constraints": {},
    }

    logger.info("ğŸ’¾ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘...")
    try:
        update_state(chat_id, {"product_options": labels})
        logger.info("âœ… ì˜µì…˜ ë¼ë²¨ ìƒíƒœ ì €ì¥ ì„±ê³µ")
    except Exception as e:
        logger.error("âŒ ì˜µì…˜ ë¼ë²¨ ì €ì¥ ì‹¤íŒ¨: %s", e)

    logger.info("ğŸ”„ ìŠ¬ë¡¯ ë³‘í•© ì¤‘...")
    merged_slots = _merge_slots(state, {"product_options": labels})
    logger.info("âœ… ì˜µì…˜ ë¼ë²¨ state ë°˜ì˜: %s", merged_slots.product_options)

    logger.info("ğŸ“¤ ìµœì¢… ê²°ê³¼ ë°˜í™˜ ì¤€ë¹„ ì¤‘...")
    tr = state.get("tool_results") or {}
    tr["option_candidates"] = option_json
    
    logger.info("ğŸ‰ ì˜µì…˜ ì œì•ˆ ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ!")
    logger.info("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:")
    logger.info("  - í›„ë³´ ìˆ˜: %d", len(candidates))
    logger.info("  - ë¼ë²¨ ëª©ë¡: %s", labels)
    logger.info("  - ì œì•½ ì¡°ê±´: %s", option_json["constraints"])
    
    return {"tool_results": tr}

def _parse_knowledge_calls(instr: Optional[Union[str, List[Dict[str, Any]], Dict[str, Any]]]) -> List[Dict[str, Any]]:
    if instr is None:
        return []

    if isinstance(instr, dict):
        return [instr]

    if isinstance(instr, list):
        # ensure list of dicts
        return [c for c in instr if isinstance(c, dict)]

    if isinstance(instr, str):
        try:
            data = json.loads(instr)
            if isinstance(data, dict):
                return [data]
            if isinstance(data, list):
                return [c for c in data if isinstance(c, dict)]
            return []
        except Exception:

            return []
    return []


def tool_executor_node(state: OrchestratorState):
    logger.info("--- ğŸ”¨ íˆ´ ì‹¤í–‰ ë…¸ë“œ ì‹¤í–‰ ---")
    instructions = state.get("instructions")
    
    if not instructions:
        logger.warning("âš ï¸ instructionsê°€ Noneì…ë‹ˆë‹¤!")
        logger.warning("ì´ì „ ë…¸ë“œì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
        logger.warning("ì‚¬ìš©ì ë©”ì‹œì§€: %s", state.get('user_message', 'N/A')[:100])
        return {"tool_results": {"error": "No instructions provided"}}
    
    tool_calls = instructions.tool_calls if instructions.tool_calls else []
    logger.info("instructions ì •ë³´: response_instruction='%s', tool_calls_count=%d", 
               instructions.response_generator_instruction[:100] if instructions.response_generator_instruction else 'N/A',
               len(tool_calls))

    if not tool_calls:
        logger.info("ì‹¤í–‰í•  íˆ´ì´ ì—†ìŠµë‹ˆë‹¤.")
        logger.info("instructions.tool_calls: %s", tool_calls)
        return {"tool_results": None}

    tool_map = {
        "t2s": lambda args: run_t2s_agent_with_instruction(state, args.get("instruction", ""), args.get("output_type", "table")),
        "tavily_search": lambda args: run_tavily_search(args.get("query", ""), args.get("max_results", 5)),
        "scrape_webpages": lambda args: scrape_webpages(args.get("urls", [])),
        "marketing_trend_search": lambda args: marketing_trend_search(args.get("question", "")),
        "beauty_youtuber_trend_search": lambda args: beauty_youtuber_trend_search(args.get("question", "")),
    }
    
    tool_results = {}

    with ThreadPoolExecutor(max_workers=len(tool_calls)) as executor:
        future_to_call = {}
        for i, call in enumerate(tool_calls):
            tool_name = call.get("tool")
            tool_args = call.get("args", {})

            logger.info(f"ğŸ§© {tool_name} ì‹¤í–‰ - args: {tool_args}")
            
            if tool_name in tool_map:
                result_key = f"{tool_name}_{i}"
                future = executor.submit(tool_map[tool_name], tool_args)
                future_to_call[future] = result_key
                logger.info(f"âœ… {tool_name} ì œì¶œ ì™„ë£Œ (result_key: {result_key})")
            else:
                logger.warning(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ '{tool_name}' í˜¸ì¶œì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                logger.warning(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {list(tool_map.keys())}")
        
        for future in future_to_call:
            result_key = future_to_call[future]

            try:
                result = future.result()
                tool_results[result_key] = result
                logger.info(f"âœ… {result_key} ì‹¤í–‰ ì™„ë£Œ")
                # ê²°ê³¼ ìš”ì•½ ë¡œê¹… (ë¯¼ê°í•œ ì •ë³´ ì œì™¸)
                if isinstance(result, dict):
                    if 'rows' in result:
                        logger.info(f"  â†’ {result_key} ë°ì´í„°: {len(result.get('rows', []))}í–‰")
                    elif 'results' in result:
                        logger.info(f"  â†’ {result_key} ê²°ê³¼: {len(result.get('results', []))}ê±´")
                    elif 'error' in result:
                        logger.warning(f"  â†’ {result_key} ë‚´ë¶€ ì—ëŸ¬: {result.get('error')}")

            except Exception as e:
                logger.error(f"âŒ '{result_key}' íˆ´ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                tool_results[result_key] = {"error": str(e)}

    logger.info(f"íˆ´ ì‹¤í–‰ ì™„ë£Œ: {len(tool_results)}ê°œ ê²°ê³¼")
    existing_results = state.get("tool_results") or {}
    merged_results = {**existing_results, **tool_results}
    
    logger.info(f"ìµœì¢… tool_results í‚¤: {list(merged_results.keys())}")
    return {"tool_results": merged_results}

def _should_visualize_router(state: OrchestratorState) -> str:
    tool_results = state.get("tool_results", {})
    # tool_results ì•ˆì— t2së¡œ ì‹œì‘í•˜ê³  ë°ì´í„°ê°€ ìˆëŠ” ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
    for key, value in tool_results.items():
        if key.startswith("t2s") and value and value.get("rows"):
            output_type = value.get("output_type", "table")
            logger.info(f"T2S ê²°ê³¼ê°€ ìˆê³  output_typeì´ '{output_type}'ì…ë‹ˆë‹¤.")
            
            # output_typeì— ë”°ë¼ ì‹œê°í™” ì—¬ë¶€ ê²°ì •
            if output_type == "visualize":
                logger.info("ì‹œê°í™”ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                return "visualize"
            elif output_type == "table":
                logger.info("í‘œë§Œ í‘œì‹œí•˜ë¯€ë¡œ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return "skip_visualize"
            elif output_type == "export":
                logger.info("íŒŒì¼ ë‚´ë³´ë‚´ê¸°ì´ë¯€ë¡œ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return "skip_visualize"
            else:
                logger.info("ê¸°ë³¸ê°’ìœ¼ë¡œ í‘œë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")
                return "skip_visualize"
            
    logger.info("T2S ê²°ê³¼ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    return "skip_visualize"
def safe_json_dumps(obj, **kwargs):
    def date_handler(obj):
        if isinstance(obj, (date, datetime)):
            return obj.isoformat()
        raise TypeError(f'Object of type {type(obj)} is not JSON serializable')
    
    return json.dumps(obj, default=date_handler, **kwargs)

def visualizer_caller_node(state: OrchestratorState):
    logger.info("--- ğŸ“Š ì‹œê°í™” ë…¸ë“œ ì‹¤í–‰ ---")
    
    # t2s ê²°ê³¼ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ì—¬ëŸ¬ tool_results ì¤‘ t2s_0, t2s_1 ë“±ì„ ì°¾ë„ë¡ ìˆ˜ì •
    t2s_result = None
    tool_results = state.get("tool_results", {})
    for key, value in tool_results.items():
        if key.startswith("t2s") and value and value.get("rows"):
            t2s_result = value
            break
    
    if not t2s_result:
        logger.info("ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {}

    # Visualizer ê·¸ë˜í”„ ì‹¤í–‰
    visualizer_app = build_visualize_graph(model="gemini-2.5-flash") # ëª¨ë¸ëª…ì€ ì„¤ì •ì— ë”°ë¼ ë³€ê²½
    viz_state = VisualizeState(
        user_question=state.get("user_message"),
        instruction="ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì•„ë˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê³  ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        json_data=safe_json_dumps(t2s_result, ensure_ascii=False)
    )
    
    viz_response = visualizer_app.invoke(viz_state)

    # ì‹œê°í™” ê²°ê³¼ë¥¼ tool_resultsì— ì¶”ê°€
    if viz_response:
        tool_results["visualization"] = {
            "json_graph": viz_response.get("json_graph"),
            "explanation": viz_response.get("output")
        }
        
    return {"tool_results": tool_results}

def promotion_final_generator(state: OrchestratorState, action_decision: dict, tr: dict) -> dict:
    """í”„ë¡œëª¨ì…˜ ìµœì¢… ê¸°íšì„œ ìƒì„± (Claude-4-Sonnet ì‚¬ìš©)"""
    logger.info("--- ğŸ¯ í”„ë¡œëª¨ì…˜ ìµœì¢… ê¸°íšì„œ ìƒì„± (Claude-4-Sonnet) ---")
    
    slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots()
    
    # ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼
    web_search = None
    marketing_trend_results = None
    youtuber_trend_results = None
    
    for key, value in tr.items():
        if key.startswith("tavily_search"): 
            web_search = value
        elif key.startswith("marketing_trend_search"):
            marketing_trend_results = value
        elif key.startswith("beauty_youtuber_trend_search"):
            youtuber_trend_results = value
    
    # íŠ¸ë Œë“œ ë°˜ì˜ ì—¬ë¶€ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    has_trends = slots.wants_trend and (web_search or marketing_trend_results or youtuber_trend_results)
    
    prompt_tmpl = textwrap.dedent(f"""
    ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ì „ëµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì™„ì„±ë„ ë†’ì€ í”„ë¡œëª¨ì…˜ ê¸°íšì„œ**ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    
    ## ğŸ“‹ í”„ë¡œëª¨ì…˜ ê¸°ë³¸ ì •ë³´
    - íƒ€ê²Ÿ ìœ í˜•: {slots.target_type}
    - í¬ì»¤ìŠ¤: {slots.focus} 
    - íƒ€ê²Ÿ ê³ ê°ì¸µ: {slots.target}
    - ì„ íƒ ìƒí’ˆ: {', '.join(slots.selected_product) if slots.selected_product else 'ì—†ìŒ'}
    - ê¸°ê°„: {slots.duration}
    - ëª©í‘œ: {slots.objective or 'ë§¤ì¶œ ì¦ëŒ€'}
    
    {"## ğŸŒŸ ìˆ˜ì§‘ëœ íŠ¸ë Œë“œ ì •ë³´" if has_trends else ""}
    {f"### ì›¹ ê²€ìƒ‰ ê²°ê³¼: {web_search}" if web_search else ""}
    {f"### ë§ˆì¼€íŒ… íŠ¸ë Œë“œ: {marketing_trend_results}" if marketing_trend_results else ""}
    {f"### ë·°í‹° íŠ¸ë Œë“œ: {youtuber_trend_results}" if youtuber_trend_results else ""}
    
    ## ğŸ¯ ì‘ì„± ìš”êµ¬ì‚¬í•­
    1. **í”„ë¡œëª¨ì…˜ ê°œìš”** (2-3ì¤„ë¡œ í•µì‹¬ ì»¨ì…‰ ìš”ì•½)
    2. **íƒ€ê²Ÿ ë¶„ì„** (ê³ ê° íŠ¹ì„±ê³¼ ë‹ˆì¦ˆ ë¶„ì„)
    3. **í•µì‹¬ ë©”ì‹œì§€** (ë¸Œëœë“œ ë©”ì‹œì§€ì™€ ì†Œêµ¬ì )
    4. **ì‹¤í–‰ ì „ëµ** (êµ¬ì²´ì ì¸ ë§ˆì¼€íŒ… ë°©ë²•ë¡ )
    {"5. **íŠ¸ë Œë“œ í™œìš©** (ìˆ˜ì§‘ëœ íŠ¸ë Œë“œë¥¼ ì–´ë–»ê²Œ í™œìš©í• ì§€)" if has_trends else ""}
    {"6." if has_trends else "5."} **ì˜ˆìƒ íš¨ê³¼** (ê¸°ëŒ€í•˜ëŠ” ì„±ê³¼ì™€ KPI)
    {"7." if has_trends else "6."} **ì‹¤í–‰ ì¼ì •** (ì£¼ìš” ë§ˆì¼ìŠ¤í†¤)
    
    ## ğŸ“ ì‘ì„± ê°€ì´ë“œë¼ì¸
    - ì‹¤ë¬´ì§„ì´ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‚´ìš©
    - ë°ì´í„°ì™€ ê·¼ê±° ê¸°ë°˜ì˜ ì „ëµì  ì‚¬ê³ 
    - ì°½ì˜ì ì´ë©´ì„œë„ ì‹¤í˜„ ê°€ëŠ¥í•œ ì•„ì´ë””ì–´
    {"- ìµœì‹  íŠ¸ë Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚¸ í˜„ëŒ€ì  ì ‘ê·¼" if has_trends else ""}
    - í•œêµ­ì–´ ì¡´ëŒ“ë§ë¡œ ì „ë¬¸ì ì´ê³  ì„¸ë ¨ëœ í†¤ì•¤ë§¤ë„ˆ
    
    ì™„ì„±ë„ ë†’ì€ í”„ë¡œëª¨ì…˜ ê¸°íšì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    """)
    
    # Claude-4-Sonnet ì‚¬ìš©
    # llm = ChatAnthropic(
    #     model="claude-sonnet-4-20250514", 
    #     temperature=0.1,
    #     max_tokens=8192,  # ë„‰ë„‰í•œ í† í° ì œí•œ ì„¤ì •
    #     api_key=settings.ANTHROPIC_API_KEY
    # )
    
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        api_key=settings.GOOGLE_API_KEY
    )
    
    final_text = llm.invoke(prompt_tmpl)
    final_response = getattr(final_text, "content", None) or str(final_text)
    
    logger.info("âœ… Claude-4-Sonnetìœ¼ë¡œ í”„ë¡œëª¨ì…˜ ê¸°íšì„œ ìƒì„± ì™„ë£Œ")
    logger.info(f"í”„ë¡œëª¨ì…˜ ê¸°íšì„œ:\n{final_response}")
    
    return final_response

def response_generator_node(state: OrchestratorState):
    logger.info("--- ğŸ—£ï¸ ì‘ë‹µ ìƒì„± ë…¸ë“œ ì‹¤í–‰ ---")
    logger.info("ì…ë ¥: user_message='%s'", state.get('user_message', 'N/A')[:100])
    instructions = state.get("instructions")
    tr = state.get("tool_results") or {}
    logger.info("instructions ì¡´ì¬: %s, tool_results í‚¤: %s", bool(instructions), list(tr.keys()))
    
    action_decision = tr.get("action")
    
    # í”„ë¡œëª¨ì…˜ ìµœì¢… ìƒì„± ìƒíƒœë“¤ì¸ì§€ í™•ì¸
    if action_decision and action_decision.get("status") in ["create_final_plan", "apply_trends"]:
        final_response = promotion_final_generator(state, action_decision, tr)
        
        history = state.get("history", [])
        history.append({"role": "user", "content": state.get("user_message", "")})
        history.append({"role": "assistant", "content": final_response})
        
        # í”„ë¡œëª¨ì…˜ ìŠ¬ë¡¯ ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜ (plan ë°ì´í„° ìƒì„±ìš©)
        slots = state.get("active_task").slots if state.get("active_task") and state.get("active_task").slots else PromotionSlots()
        
        return {
            "history": history, 
            "user_message": "", 
            "output": final_response,
            "promotion_slots": slots.model_dump(),
            "is_final_promotion": True
        }

    # ê¸°ì¡´ ë¡œì§ (Gemini ì‚¬ìš©)
    instructions_text = (
        instructions.response_generator_instruction
        if instructions and instructions.response_generator_instruction
        else "ì‚¬ìš©ì ìš”ì²­ì— ë§ì¶° ì •ì¤‘í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    )

    t2s_table = None
    t2s_output_type = "table"  # ê¸°ë³¸ê°’
    t2s_download_url = None
    web_search = None
    scraped_pages = None
    marketing_trend_results = None
    youtuber_trend_results = None
    for key, value in tr.items():
        if key.startswith("t2s") and isinstance(value, dict) and "rows" in value:
            t2s_table = value
            t2s_output_type = value.get("output_type", "table")
            t2s_download_url = value.get("download_url")
        elif key.startswith("tavily_search"): 
            web_search = value
        elif key.startswith("scrape_webpages"):
            scraped_pages = value
        elif key.startswith("marketing_trend_search"):
            marketing_trend_results = value
        elif key.startswith("beauty_youtuber_trend_search"):
            youtuber_trend_results = value

    action_decision = tr.get("action")
    knowledge_snippet = tr.get("knowledge") if isinstance(tr.get("knowledge"), str) else None
    option_candidates = tr.get("option_candidates") if isinstance(tr.get("option_candidates"), dict) else None

    prompt_tmpl = textwrap.dedent("""
    ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì˜ ìµœì¢… ì‘ë‹µ ìƒì„±ê¸°ì…ë‹ˆë‹¤.
    ì•„ë˜ ì…ë ¥ë§Œì„ ê·¼ê±°ë¡œ **í•œêµ­ì–´ ì¡´ëŒ“ë§**ë¡œ í•œ ë²ˆì— ì™„ì„±ëœ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    ë‚´ë¶€ ë„êµ¬ëª…ì´ë‚˜ ì‹œìŠ¤í…œ ì„¸ë¶€ êµ¬í˜„ì€ ì–¸ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    [ì…ë ¥ ì„¤ëª…]
    - instructions_text: ì´ë²ˆ í„´ì˜ í†¤/ë°©í–¥.
    - action_decision: í”„ë¡œëª¨ì…˜ ì˜ì‚¬ê²°ì •(JSON).
    - option_candidates: ìœ ì €ì—ê²Œ ì œì•ˆí•  í›„ë³´ ëª©ë¡(JSON).
    - t2s_table: DB ì§ˆì˜ ê²°ê³¼(JSON). ìˆìœ¼ë©´ ìƒìœ„ 10í–‰ë§Œ í‘œë¡œ ë¯¸ë¦¬ë³´ê¸°.
    - knowledge_snippet: ê°„ë‹¨ ì°¸ê³ (ì„ íƒ).
    - web_search: ì›¹ ê²€ìƒ‰ ê²°ê³¼(JSON: results[title,url,content]).
    - scraped_pages: ì›¹ í˜ì´ì§€ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ê²°ê³¼(JSON: documents[source,content]).
    - marketing_trend_results: Supabase ë§ˆì¼€íŒ… íŠ¸ë Œë“œ ê²°ê³¼(JSON).
    - youtuber_trend_results: Supabase ë·°í‹° ìœ íŠœë²„ íŠ¸ë Œë“œ ê²°ê³¼(JSON).

    [ì‘ì„± ì§€ì¹¨]
    1) **ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™**: `action_decision` ê°ì²´ê°€ ìˆê³ , ê·¸ ì•ˆì˜ `ask_prompts` ë¦¬ìŠ¤íŠ¸ì— ë‚´ìš©ì´ ìˆë‹¤ë©´, ë‹¹ì‹ ì˜ ìµœìš°ì„  ì„ë¬´ëŠ” í•´ë‹¹ ë¦¬ìŠ¤íŠ¸ì˜ ì§ˆë¬¸ì„ ì‚¬ìš©ìì—ê²Œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë“  ì§€ì‹œë³´ë‹¤ ì´ ê·œì¹™ì„ **ë°˜ë“œì‹œ** ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. `ask_prompts`ì˜ ë¬¸êµ¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜, ì‚´ì§ ë” ìì—°ìŠ¤ëŸ½ê²Œë§Œ ë‹¤ë“¬ì–´ ì§ˆë¬¸í•˜ì„¸ìš”.
    1-1) **ì¤‘ë³µ ì§ˆë¬¸ ë°©ì§€**: ì´ë¯¸ ì±„ì›Œì§„ ìŠ¬ë¡¯ì— ëŒ€í•´ì„œëŠ” ì ˆëŒ€ ì¬ì§ˆë¬¸í•˜ì§€ ë§ˆì„¸ìš”. ask_promptsì— ìˆì–´ë„ ì´ë¯¸ ë‹µë³€ëœ ë‚´ìš©ì´ë©´ ê±´ë„ˆë›°ì„¸ìš”.
    1-2) **ì§„í–‰ ìƒí™© ì •í™•íˆ íŒŒì•…**: action_decisionì˜ payloadë‚˜ missing_slotsë¥¼ ë³´ê³  í˜„ì¬ ëª‡ ë²ˆì§¸ ì§ˆë¬¸ì¸ì§€ ì •í™•íˆ íŒë‹¨í•˜ì„¸ìš”. ì²« ì§ˆë¬¸ì´ë©´ "í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘í–ˆë‹¤"ëŠ” ì‹ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    1-3) **"ë§ˆì§€ë§‰" í‘œí˜„ ì£¼ì˜**: í”„ë¡œëª¨ì…˜ ì§ˆë¬¸ ìˆœì„œëŠ” â‘ í”„ë¡œëª¨ì…˜ì¢…ë¥˜ â‘¡ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ â‘¢ê¸°ê°„ â‘£ìƒí’ˆì„ íƒ â‘¤íŠ¸ë Œë“œë°˜ì˜ ì…ë‹ˆë‹¤. íŠ¸ë Œë“œë°˜ì˜ ì§ˆë¬¸ì„ í•  ë•Œë§Œ "ë§ˆì§€ë§‰ìœ¼ë¡œ"ë¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
    2) **í”„ë¡œëª¨ì…˜ ì™„ì„± ê·œì¹™**: 
       - `action_decision`ì˜ `status`ê°€ "start_promotion"ì¸ ê²½ìš°, ì™„ì„±ëœ í”„ë¡œëª¨ì…˜ ìŠ¬ë¡¯ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡œëª¨ì…˜ ë‚´ìš©ì„ ì •ë¦¬í•´ì„œ ë³´ì—¬ì£¼ê³ , ë§ˆì§€ë§‰ ë¬¸ë‹¨ì— ë°˜ë“œì‹œ "ìµœì‹  íŠ¸ë Œë“œë‚˜ ìœ í–‰ì–´ë¥¼ ë°˜ì˜í•´ì„œ í”„ë¡œëª¨ì…˜ì„ ë§Œë“¤ê¸¸ ì›í•˜ì‹œë‚˜ìš”?"ë¼ê³  ì§ˆë¬¸í•˜ì„¸ìš”.
       - `action_decision`ì˜ `status`ê°€ "create_final_plan"ì¸ ê²½ìš°, íŠ¸ë Œë“œ ë°˜ì˜ ì—†ì´ ì™„ì„±ëœ í”„ë¡œëª¨ì…˜ ê¸°íšì„œë¥¼ ì œì‘í•˜ì„¸ìš”.
       - `action_decision`ì˜ `status`ê°€ "apply_trends"ì´ê³  ì™¸ë¶€ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°, ìˆ˜ì§‘ëœ íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•œ ìµœì¢… í”„ë¡œëª¨ì…˜ ê¸°íšì„œë¥¼ ì œì‘í•˜ì„¸ìš”.
       - **ì¤‘ìš”**: ì‚¬ìš©ìê°€ ì´ë¯¸ íŠ¸ë Œë“œ ë°˜ì˜ ì—¬ë¶€ì— ëŒ€í•´ ë‹µë³€í–ˆë‹¤ë©´ (wants_trendê°€ true/falseë¡œ ì„¤ì •ë¨), ê°™ì€ ì§ˆë¬¸ì„ ë‹¤ì‹œ í•˜ì§€ ë§ˆì„¸ìš”.
    3) ìœ„ 1,2ë²ˆ ê·œì¹™ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ, `instructions_text`ë¥¼ ì£¼ëœ ë‚´ìš©ìœ¼ë¡œ ì‚¼ì•„ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    4) **í”„ë¡œëª¨ì…˜ í•„ë“œ ì§ˆë¬¸ ê·œì¹™**: 
       - í•„ìˆ˜ í•„ë“œ: target_type, focus, duration, selected_product (ë°˜ë“œì‹œ ë¬¼ì–´ë´ì•¼ í•¨)
       - wants_trend: íŠ¸ë Œë“œ ë°˜ì˜ ì§ˆë¬¸ë§Œ (ì‚¬ìš©ìê°€ ë¨¼ì € ì–¸ê¸‰í•˜ì§€ ì•ŠëŠ” í•œ êµ³ì´ ë¬¼ì–´ë³´ì§€ ë§ˆì„¸ìš”)
       - objective: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬ (êµ³ì´ ì§ˆë¬¸í•˜ì§€ ë§ˆì„¸ìš”)
       - ê¸ˆì§€ í•„ë“œ: budget, cost, ì˜ˆì‚° ë“± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í•„ë“œë“¤)
       - **ì¤‘ìš”**: missing_slots ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ì„œ ë‚¨ì€ í•„ë“œê°€ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ íŒŒì•…í•˜ê³ , ì ì ˆí•œ í†¤ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”.
    5) `option_candidates`ê°€ ìˆìœ¼ë©´ ë²ˆí˜¸ë¡œ ì œì‹œí•˜ê³  ê° 2~4ì¤„ ê·¼ê±°ë¥¼ ë¶™ì…ë‹ˆë‹¤. 
       - í›„ë³´ì— `llm_reasons` í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©í•˜ì„¸ìš” (LLMì´ ìƒì„±í•œ ìƒì„¸ ê·¼ê±°)
       - `llm_reasons`ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ `reasons`, `business_reasons` ë“±ì„ ì‚¬ìš©í•˜ì„¸ìš”
       - ëª¨ë“  ìˆ˜ì¹˜ëŠ” ì–´ë–¤ ìˆ˜ì¹˜ì¸ì§€ êµ¬ì²´ì ì¸ ì–¸ê¸‰ì„ í•´ì£¼ì„¸ìš”
       - ë§ˆì§€ë§‰ì— 'ê¸°íƒ€(ì§ì ‘ ì…ë ¥)'ë„ ì¶”ê°€í•©ë‹ˆë‹¤    
    6) web_search / scraped_pages / supabase ê²°ê³¼ê°€ ìˆìœ¼ë©´, í•µì‹¬ ê·¼ê±°ë¥¼ 2~4ì¤„ë¡œ ìš”ì•½í•´ ì„¤ëª…ì— ë…¹ì—¬ ì£¼ì„¸ìš”. ì›ë¬¸ ì¸ìš©ì€ 1~2ë¬¸ì¥ ì´í•˜ë¡œ ì œí•œ.
    7) t2s_table ì²˜ë¦¬ ê·œì¹™:
       - output_typeì´ "export"ì¸ ê²½ìš°: í‘œë‚˜ ì‹œê°í™”ë¥¼ í¬í•¨í•˜ì§€ ë§ê³ , ë°ì´í„° ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŒì„ ì•ˆë‚´í•˜ì„¸ìš”. ë‹¤ìš´ë¡œë“œ ë§í¬ëŠ” ì‹œìŠ¤í…œì—ì„œ ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.
       - output_typeì´ "table"ì¸ ê²½ìš°: ìƒìœ„ 10í–‰ ë¯¸ë¦¬ë³´ê¸° í‘œë§Œ í¬í•¨í•˜ë˜, ì—†ëŠ” ìˆ˜ì¹˜ëŠ” ë§Œë“¤ì§€ ë§ˆì„¸ìš”. í‘œë¥¼ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ì€ [TABLE_START] í‘œê°€ ëë‚˜ëŠ” ë¶€ë¶„ì€ [TABLE_END] ë¼ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ì„œ ì–´ë””ë¶€í„° ì–´ë””ê°€ í…Œì´ë¸”ì¸ì§€ ì•Œ ìˆ˜ ìˆê²Œ í•´ì£¼ì„¸ìš”.
       - output_typeì´ "visualize"ì¸ ê²½ìš°: ìƒìœ„ 10í–‰ ë¯¸ë¦¬ë³´ê¸° í‘œë¥¼ í¬í•¨í•˜ê³ , ì‹œê°í™” ê²°ê³¼ê°€ ìˆë‹¤ë©´ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”.
    8) ì „ì²´ì ìœ¼ë¡œ êµ¬ì¡°í™”ëœ í˜•ì‹ì„ ìœ ì§€í•˜ì„¸ìš”.

    [ì…ë ¥ ë°ì´í„°]
    - instructions_text:
    {instructions_text}

    - action_decision (JSON):
    {action_decision_json}

    - option_candidates (JSON):
    {option_candidates_json}

    - t2s_table (JSON):
    {t2s_table_json}

    - t2s_output_type:
    {t2s_output_type}



    - web_search (JSON):
    {web_search_json}

    - scraped_pages (JSON):
    {scraped_pages_json}

    - marketing_trend_results (JSON):
    {marketing_trend_results_json}

    - youtuber_trend_results (JSON):
    {youtuber_trend_results_json}

    - knowledge_snippet:
    {knowledge_snippet}
    """)

    def to_json(x):
        if x is None:
            return "null"
        try:
            return json.dumps(x, ensure_ascii=False, default=str)
        except Exception as e:
            logger.warning(f"JSON ì§ë ¬í™” ì‹¤íŒ¨: {e}, ë¹ˆ ê°ì²´ë¡œ ì²˜ë¦¬")
            return "{}"

    prompt = ChatPromptTemplate.from_template(prompt_tmpl)
    # llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0, api_key=settings.GOOGLE_API_KEY)
    # llm = ChatAnthropic(
    #     model="claude-sonnet-4-20250514", 
    #     temperature=0.1,
    #     max_tokens=8192,  # ë„‰ë„‰í•œ í† í° ì œí•œ ì„¤ì •
    #     api_key=settings.ANTHROPIC_API_KEY
    # )

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        api_key=settings.GOOGLE_API_KEY
    )
        
    final_text = (prompt | llm).invoke({
        "instructions_text": instructions_text,
        "action_decision_json": to_json(action_decision),
        "option_candidates_json": to_json(option_candidates),
        "t2s_table_json": to_json(t2s_table),
        "t2s_output_type": t2s_output_type,

        "web_search_json": to_json(web_search),
        "scraped_pages_json": to_json(scraped_pages),
        "marketing_trend_results_json": to_json(marketing_trend_results),
        "youtuber_trend_results_json": to_json(youtuber_trend_results),
        "knowledge_snippet": knowledge_snippet or "",
    })

    # AIMessage ê°ì²´ ì•ˆì „ ì²˜ë¦¬
    if hasattr(final_text, "content"):
        final_response = final_text.content
    else:
        final_response = str(final_text)
    
    logger.info(f"âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(final_response)}ì)")
    logger.info(f"ìµœì¢… ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {final_response}...")
    
    history = state.get("history", [])
    history.append({"role": "user", "content": state.get("user_message", "")})
    history.append({"role": "assistant", "content": final_response})
    
    logger.info(f"íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸: ì´ {len(history)}ê°œ ë©”ì‹œì§€")
    return {"history": history, "user_message": "", "output": final_response}

# ===== Graph =====
workflow = StateGraph(OrchestratorState)

workflow.add_node("planner", planner_node)
workflow.add_node("slot_extractor", slot_extractor_node)
workflow.add_node("action_state", action_state_node)
workflow.add_node("options_generator", options_generator_node)
workflow.add_node("tool_executor", tool_executor_node)
workflow.add_node("visualizer", visualizer_caller_node) 
workflow.add_node("response_generator", response_generator_node)

workflow.set_entry_point("planner")

workflow.add_conditional_edges(
    "planner",
    _planner_router,
    {
        "slot_extractor": "slot_extractor",
        "tool_executor": "tool_executor",
        "response_generator": "response_generator",
    },
)

workflow.add_edge("slot_extractor", "action_state")
workflow.add_conditional_edges(
    "action_state",
    _action_router,
    {
        "options_generator": "options_generator",
        "response_generator": "response_generator",
        "tool_executor": "tool_executor",
    },
)
workflow.add_edge("options_generator", "response_generator")

workflow.add_conditional_edges(
    "tool_executor",
    _should_visualize_router,
    {
        "visualize": "visualizer",
        "skip_visualize": "response_generator"
    }
)
workflow.add_edge("visualizer", "response_generator")
workflow.add_edge("response_generator", END)

orchestrator_app = workflow.compile()