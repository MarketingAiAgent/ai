import logging 
import json 

from langchain_tavily import TavilySearch
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

from app.database.supabase import supabase_client, embeddings
from app.core.config import settings 

from app.agents.text_to_sql.__init__ import call_sql_generator
from .state import *
from .helpers import *

logger = logging.getLogger(__name__)

_tavily = TavilySearch(max_results=5)

def run_t2s_agent_with_instruction(state: OrchestratorState, instruction: str): 
    result = call_sql_generator(
        message=instruction, 
        conn_str=state["conn_str"], 
        schema_info=state["schema_info"]
    )
    table = result.get("data_json")
    if isinstance(table, str):
        try:
            table = json.loads(table)
        except Exception:
            table = {"rows": [], "columns": [], "row_count": 0}
    return ensure_table_payload(table)

def run_tavily_search(query: str, max_results: int = 5) -> Dict[str, Any]:
    """
    ê°„ë‹¨í•œ ì›¹ ê²€ìƒ‰. ê²°ê³¼ ìŠ¤í‚¤ë§ˆëŠ” ì•„ë˜ í˜•íƒœë¡œ ê³ ì •:
    { "results": [ {"title":..., "url":..., "content":...}, ... ] }
    """
    try:
        tool = TavilySearch(max_results=max_results, ) if max_results != 5 else _tavily
        out = tool.invoke(query) 
        if isinstance(out, str):
            try:
                parsed_out = json.loads(out)
                out = parsed_out
                
            except json.JSONDecodeError:
                logger.error("Tavilyê°€ ì¼ë°˜ ì—ëŸ¬ ë¬¸ìì—´ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: %s", out)
                return {"results": [], "error": out}

        results = []
        for r in out.get('results') or []:
            results.append({
                "title": r.get("title"),
                "url": r.get("url"),
                "content": r.get("content"),
            })
        return {"results": results}
    except Exception as e:
        logger.error("Tavily ê²€ìƒ‰ ì‹¤íŒ¨: %s", e, "ì¶œë ¥:", out)
        return {"results": [], "error": str(e)}

def scrape_webpages(urls: List[str]) -> Dict[str, Any]:
    """
    ì£¼ì–´ì§„ URL ëª©ë¡ì—ì„œ ë³¸ë¬¸ì„ ê°€ì ¸ì™€ í•©ì¹©ë‹ˆë‹¤.
    ë°˜í™˜ ìŠ¤í‚¤ë§ˆ:
    { "documents": [ {"source": <title or url>, "content": <text>}, ... ] }
    """
    if not urls:
        return {"documents": []}
    if WebBaseLoader is None:
        return {"documents": [], "error": "WebBaseLoader not available"}

    try:
        loader = WebBaseLoader(
            web_path=urls,
            header_template={
                "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                               "AppleWebKit/537.36 (KHTML, like Gecko) "
                               "Chrome/102.0.0.0 Safari/537.36"),
            },
        )
        docs = loader.load()
        items = []
        for d in docs:
            src = d.metadata.get("title") or d.metadata.get("source") or d.metadata.get("url") or ""
            items.append({"source": src, "content": d.page_content})
        return {"documents": items}
    except Exception as e:
        logger.error("ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: %s", e)
        return {"documents": [], "error": str(e)}

def marketing_trend_search(question: str) -> Dict[str, Any]:
    """
    Supabase í•¨ìˆ˜ 'marketing_vector_search' í˜¸ì¶œ.
    ë°˜í™˜ ìŠ¤í‚¤ë§ˆ:
    { "results": [ {"title":..,"chunk_text":..,"text":..,"subtitle":..}, ... ] }
    """
    logger.info("ğŸ” ë§ˆì¼€íŒ… íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹œì‘ - ì§ˆë¬¸: %s", question)
    
    if not (supabase_client and embeddings):
        logger.error("âŒ Supabase í´ë¼ì´ì–¸íŠ¸ ë˜ëŠ” ì„ë² ë”© ë¯¸ì„¤ì •")
        return {"results": [], "error": "supabase_client/embeddings not configured"}
    
    try:
        # ì„ë² ë”© ìƒì„±
        logger.info("ğŸ¤– ì„ë² ë”© ë²¡í„° ìƒì„± ì¤‘...")
        vec = embeddings.embed_query(question)
        logger.info("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ - ë²¡í„° ì°¨ì›: %d", len(vec))
        
        # Supabase RPC í˜¸ì¶œ
        logger.info("ğŸ“ Supabase marketing_vector_search í•¨ìˆ˜ í˜¸ì¶œ ì¤‘...")
        resp = supabase_client.rpc("marketing_vector_search", {"query_vector": vec}).execute()
        logger.info("âœ… Supabase í˜¸ì¶œ ì™„ë£Œ")
        
        # ì‘ë‹µ ë°ì´í„° í™•ì¸
        raw_data = resp.data or []
        logger.info("ğŸ“Š ì‘ë‹µ ë°ì´í„° ê°œìˆ˜: %d", len(raw_data))
        
        if not raw_data:
            logger.warning("âš ï¸ Supabaseì—ì„œ ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return {"results": []}
        
        # ê²°ê³¼ ì²˜ë¦¬
        out = []
        for i, it in enumerate(raw_data):
            logger.debug("ì²˜ë¦¬ ì¤‘ %dë²ˆì§¸ ì•„ì´í…œ: %s", i+1, list(it.keys()) if isinstance(it, dict) else type(it))
            out.append({
                "title": it.get("title", "ì œëª© ì—†ìŒ"),
                "chunk_text": it.get("chunk_text", ""),
                "text": it.get("text", ""),
                "subtitle": it.get("subtitle", ""),
            })
        
        logger.info("ğŸ‰ ë§ˆì¼€íŒ… íŠ¸ë Œë“œ ê²€ìƒ‰ ì™„ë£Œ - ìµœì¢… ê²°ê³¼: %dê±´", len(out))
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ ìƒ˜í”Œ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        if out:
            first_result = out[0]
            logger.info("ğŸ“‹ ì²« ë²ˆì§¸ ê²°ê³¼ ìƒ˜í”Œ: title='%s', chunk_length=%d", 
                       first_result.get("title", "")[:50], 
                       len(first_result.get("chunk_text", "")))
        
        return {"results": out}
        
    except Exception as e:
        logger.error("âŒ Supabase marketing search ì‹¤íŒ¨: %s", e)
        logger.error("ì—ëŸ¬ íƒ€ì…: %s", type(e).__name__)
        if hasattr(e, 'response'):
            logger.error("HTTP ì‘ë‹µ ìƒíƒœ: %s", getattr(e.response, 'status_code', 'N/A'))
        return {"results": [], "error": str(e)}

def beauty_youtuber_trend_search(question: str, summarize=True) -> Dict[str, Any]:
    """
    Supabase í•¨ìˆ˜ 'beauty_vector_search' í˜¸ì¶œ.
    ìŠ¤í‚¤ë§ˆëŠ” marketing_trend_searchì™€ ë™ì¼.
    """
    if not (supabase_client and embeddings):
        return {"results": [], "error": "supabase_client/embeddings not configured"}
    try:
        vec = embeddings.embed_query(question)
        resp = supabase_client.rpc("beauty_vector_search", {
            "query_vector": vec}).execute()
        out = []
        for it in resp.data or []:
            out.append({
                "title": it.get("title", "ì œëª© ì—†ìŒ"),
                "chunk_text": it.get("chunk_text", ""),
                "text": it.get("text", ""),
                "subtitle": it.get("subtitle", ""),
            })
        if not summarize: 
            return {'results': out}

        llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0, api_key=settings.GOOGLE_API_KEY)

        SUMMARIZER_PROMPT = """
You are a research assistant. Your task is to analyze the raw text from a tool's output and summarize the key findings that are directly relevant to the user's original question.

User's original question: "{question}"

Raw text from the tool:
---
{raw_text}
---

Based on the raw text, please extract and summarize only the essential information that answers the user's question.
Present the summary in a few concise bullet points in Korean.
If the raw text contains no relevant information to answer the question, just return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
""" 
        prompt = ChatPromptTemplate.from_template(SUMMARIZER_PROMPT)

        summary_chain = prompt | llm
        summary = summary_chain.invoke({
                "question": question,
                "raw_text": out
            }).content
        
        logger.info(f"ë·°í‹° ìœ íŠœë²„ í™•ì¸ ì™„ë£Œ: {summary}")
            
        return {"results": summary}
    except Exception as e:
        logger.error("Supabase beauty youtuber search ì‹¤íŒ¨: %s", e)
        return {"results": [], "error": str(e)}

STOPWORDS = {
    "íŠ¸ë Œë“œ","ë§ˆì¼€íŒ…","ë¸Œëœë“œ","ì œí’ˆ","ìƒí’ˆ","ë¦¬ë·°","ì¶”ì²œ","ìµœì‹ ","ë¶„ì„","ì‚¬ë¡€","ì „ëµ","ê´‘ê³ ",
    "ì¼€ì´ìŠ¤","ë‰´ìŠ¤","ì´ìŠˆ","ì˜ìƒ","ì±„ë„","ì»¨í…ì¸ ","ì½˜í…ì¸ ","ìº í˜ì¸","í”„ë¡œëª¨ì…˜","ì—…ê³„","ì‹œì¥",
    "ì˜¬í•´","ì´ë²ˆ","ìµœê·¼","í•µì‹¬","ì†Œê°œ","ì •ë¦¬","ê°€ì´ë“œ","ë°©ë²•","ì´ìœ ","í¬ì¸íŠ¸","ì‚¬ìš©","í™œìš©",
    "and","or","of","for","to","in","on","with","by","the","a","an"
}

HASHTAG_RE = re.compile(r"#([0-9A-Za-zê°€-í£_]{2,30})")
QUOTED_RE  = re.compile(r"[\"â€œâ€â€˜â€™']([^\"â€œâ€â€˜â€™']{2,30})[\"â€œâ€â€˜â€™']")
HANGUL_TERM_RE = re.compile(r"[ê°€-í£]{2,8}")
EN_TERM_RE = re.compile(r"[A-Za-z][A-Za-z0-9\-\+]{2,20}")

SEASON_KEYWORDS = {
    "ì¶”ì„":  {"window": "YYYY-09-05~YYYY-09-25", "term": "ì¶”ì„ì„ ë¬¼"},
    "ì„¤ë‚ ":  {"window": "YYYY-01-15~YYYY-02-20", "term": "ì„¤ ì„ ë¬¼"},
    "ì—¬ë¦„":  {"window": "YYYY-06-15~YYYY-08-31", "term": "ì—¬ë¦„íœ´ê°€"},
    "ê²¨ìš¸":  {"window": "YYYY-11-15~YYYY-12-31", "term": "ì—°ë§/ê²¨ìš¸"},
    "ë´„":    {"window": "YYYY-03-01~YYYY-05-31", "term": "ë´„ ì‹ ì œí’ˆ"},
    "ê°€ì„":  {"window": "YYYY-09-01~YYYY-10-31", "term": "ê°€ì„ í”„ë¡œëª¨ì…˜"},
}

def _normalize_term(t: str) -> str:
    t = t.strip().strip(".,!?()[]{}:;â€¦Â·ãƒ»/\\|")
    return t

def _extract_terms(text: str) -> List[str]:
    if not text:
        return []
    terms = []
    # í•´ì‹œíƒœê·¸ / ë”°ì˜´í‘œ / í•œê¸€ ì–´ì ˆ / ì˜ë¬¸ í† í°
    terms += [m.group(1) for m in HASHTAG_RE.finditer(text)]
    terms += [m.group(1) for m in QUOTED_RE.finditer(text)]
    terms += HANGUL_TERM_RE.findall(text)
    terms += EN_TERM_RE.findall(text)
    out = []
    for t in terms:
        t = _normalize_term(t)
        if not t or t.lower() in STOPWORDS or t in STOPWORDS:
            continue
        # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œê±°
        if len(t) < 2:
            continue
        out.append(t)
    return out

def _rank_terms(corpus: List[str], top_k: int = 8) -> List[str]:
    from collections import Counter
    cnt = Counter([t for t in corpus if t and t.lower() not in STOPWORDS])
    # í•œê¸€/ì˜ë¬¸ ì„ì—¬ ìˆì„ ë•Œ ê¸¸ì´/ë¹ˆë„ ê°€ì¤‘ ê°„ë‹¨ ì ìš©
    ranked = sorted(cnt.items(), key=lambda kv: (kv[1], len(kv[0])), reverse=True)
    res: List[str] = []
    seen = set()
    for term, _ in ranked:
        key = term.lower()
        if key in seen:
            continue
        seen.add(key)
        res.append(term)
        if len(res) >= top_k:
            break
    return res

def _guess_seasonal_spikes(texts: List[str], year: Optional[int] = None) -> List[Dict[str, str]]:
    """
    ë¬¸ì„œ í…ìŠ¤íŠ¸ì—ì„œ ê³„ì ˆ/ëª…ì ˆ í‚¤ì›Œë“œë¥¼ ë°œê²¬í•˜ë©´ ëŒ€ëµì˜ ìœˆë„ìš°ë¥¼ ë°˜í™˜.
    YYYYëŠ” ì‹¤ì œ ì—°ë„ë¡œ êµì²´í•˜ì§€ ì•Šê³  ê³ ì • ë¬¸ìì—´ë¡œ ë‘¡ë‹ˆë‹¤(ì†Œë¹„ ì¸¡ì—ì„œ ì¹˜í™˜ ê°€ëŠ¥).
    """
    found = set()
    spikes: List[Dict[str, str]] = []
    all_txt = " ".join(texts)
    for k, v in SEASON_KEYWORDS.items():
        if k in all_txt:
            if k not in found:
                found.add(k)
                spikes.append({"term": v["term"], "window": v["window"]})
    return spikes

def get_knowledge_snapshot(
    topic: Optional[str] = None,
    *,
    use_web: bool = True,
    use_supabase: bool = True,
    max_results: int = 5,
    scrape_k: int = 3,
) -> Dict[str, Any]:
    """
    ì™¸ë¶€ ì‹ í˜¸ë¥¼ í•©ì³ 'íŠ¸ë Œë“œ ìŠ¤ëƒ…ìƒ·'ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë°˜í™˜ ìŠ¤í‚¤ë§ˆ:
    {
      "trending_terms": [str, ...],
      "seasonal_spikes": [{"term": str, "window": "YYYY-MM-DD~YYYY-MM-DD"}, ...],
      "notes": [str, ...],
      "sources": [{"title": str, "url": str}],
      "raw": { "web_search": ..., "scraped_pages": ..., "marketing": ..., "youtuber": ... }  # (ì˜µì…˜)
    }
    """
    logger.info("ğŸ” ì§€ì‹ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ ì‹œì‘")
    logger.info("ğŸ“‹ ì…ë ¥ íŒŒë¼ë¯¸í„°:")
    logger.info("  - topic: %s", topic)
    logger.info("  - use_web: %s", use_web)
    logger.info("  - use_supabase: %s", use_supabase)
    logger.info("  - max_results: %d", max_results)
    logger.info("  - scrape_k: %d", scrape_k)
    
    # 0) ê¸°ë³¸ ì¿¼ë¦¬
    query = topic or "í•œêµ­ ì†Œë¹„ì íŠ¸ë Œë“œ 2025 ìˆí¼ ë°”ì´ëŸ´ ë§ˆì¼€íŒ…"
    logger.info("ğŸ¯ ìµœì¢… ê²€ìƒ‰ ì¿¼ë¦¬: %s", query)
    
    all_texts: List[str] = []
    sources: List[Dict[str, str]] = []
    notes: List[str] = []

    web_search_res = None
    scraped_res = None
    mk_res = None
    yt_res = None

    # 1) ì›¹ ê²€ìƒ‰
    if use_web:
        logger.info("ğŸŒ ì›¹ ê²€ìƒ‰ ë‹¨ê³„ ì‹œì‘...")
        web_search_res = run_tavily_search(query, max_results=max_results)
        
        logger.info("ğŸ“Š ì›¹ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„:")
        web_results = web_search_res.get("results") or []
        logger.info("  - ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: %d", len(web_results))
        
        if web_search_res.get("error"):
            logger.warning("âš ï¸ ì›¹ ê²€ìƒ‰ ì—ëŸ¬: %s", web_search_res.get("error"))
        
        for i, r in enumerate(web_results):
            title = r.get("title") or ""
            url = r.get("url") or ""
            content = r.get("content") or ""
            sources.append({"title": title, "url": url})
            
            logger.info("  %dë²ˆ ê²°ê³¼: %s", i+1, title[:50] + "..." if len(title) > 50 else title)
            logger.info("    - URL: %s", url)
            logger.info("    - ì½˜í…ì¸  ê¸¸ì´: %dì", len(content))
            
            # íƒ€ì´í‹€/ìŠ¤ë‹ˆí«ë§Œ ë¨¼ì € ìˆ˜ì§‘
            if title: 
                all_texts.append(title)
                logger.debug("    - íƒ€ì´í‹€ ì¶”ê°€ë¨")
            if content: 
                all_texts.append(content)
                logger.debug("    - ì½˜í…ì¸  ì¶”ê°€ë¨")

        logger.info("ğŸ“š ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ìˆ˜: %d", len(all_texts))

        # 2) ìŠ¤í¬ë©(ìµœëŒ€ scrape_kê°œ)
        urls = [s["url"] for s in sources if s.get("url")] if sources else []
        logger.info("ğŸ”— ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ URL ìˆ˜: %d", len(urls))
        
        if urls:
            logger.info("ğŸ“„ ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
            scraped_res = scrape_webpages(urls[:scrape_k])
            
            scraped_docs = scraped_res.get("documents") or []
            logger.info("ğŸ“„ ìŠ¤í¬ë˜í•‘ ê²°ê³¼:")
            logger.info("  - ìŠ¤í¬ë˜í•‘ëœ ë¬¸ì„œ ìˆ˜: %d", len(scraped_docs))
            
            if scraped_res.get("error"):
                logger.warning("âš ï¸ ìŠ¤í¬ë˜í•‘ ì—ëŸ¬: %s", scraped_res.get("error"))
            
            for i, d in enumerate(scraped_docs):
                if d.get("content"):
                    content_length = len(d["content"])
                    all_texts.append(d["content"])
                    logger.info("  %dë²ˆ ë¬¸ì„œ: %s (ê¸¸ì´: %dì)", i+1, d.get("source", "Unknown")[:50], content_length)
        else:
            logger.info("âš ï¸ ìŠ¤í¬ë˜í•‘í•  URLì´ ì—†ìŠµë‹ˆë‹¤")

    # 3) Supabase ë§ˆì¼€íŒ…/ë·°í‹° ì¸ì‚¬ì´íŠ¸
    if use_supabase:
        logger.info("ğŸ—„ï¸ Supabase ê²€ìƒ‰ ë‹¨ê³„ ì‹œì‘...")
        
        logger.info("ğŸ“ˆ ë§ˆì¼€íŒ… íŠ¸ë Œë“œ ê²€ìƒ‰ ì¤‘...")
        mk_res = marketing_trend_search(query)
        
        mk_results = mk_res.get("results") or []
        logger.info("ğŸ“Š ë§ˆì¼€íŒ… ê²€ìƒ‰ ê²°ê³¼:")
        logger.info("  - ë§ˆì¼€íŒ… ê²°ê³¼ ìˆ˜: %d", len(mk_results))
        
        if mk_res.get("error"):
            logger.warning("âš ï¸ ë§ˆì¼€íŒ… ê²€ìƒ‰ ì—ëŸ¬: %s", mk_res.get("error"))
        
        for i, item in enumerate(mk_results):
            for k in ("title", "chunk_text", "text", "subtitle"):
                if item.get(k): 
                    all_texts.append(item[k])
                    logger.debug("  %dë²ˆ ë§ˆì¼€íŒ… ê²°ê³¼ %s í•„ë“œ ì¶”ê°€ë¨", i+1, k)
        
        logger.info("ğŸ’„ ë·°í‹° ìœ íŠœë²„ íŠ¸ë Œë“œ ê²€ìƒ‰ ì¤‘...")
        yt_res = beauty_youtuber_trend_search(query, summarize=False)
        
        yt_results = yt_res.get("results") or []
        logger.info("ğŸ“Š ë·°í‹° ìœ íŠœë²„ ê²€ìƒ‰ ê²°ê³¼:")
        logger.info("  - ë·°í‹° ìœ íŠœë²„ ê²°ê³¼ ìˆ˜: %d", len(yt_results))
        
        if yt_res.get("error"):
            logger.warning("âš ï¸ ë·°í‹° ìœ íŠœë²„ ê²€ìƒ‰ ì—ëŸ¬: %s", yt_res.get("error"))
        
        for i, item in enumerate(yt_results):
            for k in ("title", "chunk_text", "text", "subtitle"):
                if item.get(k): 
                    all_texts.append(item[k])
                    logger.debug("  %dë²ˆ ë·°í‹° ìœ íŠœë²„ ê²°ê³¼ %s í•„ë“œ ì¶”ê°€ë¨", i+1, k)

    logger.info("ğŸ“š ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ:")
    logger.info("  - ì´ í…ìŠ¤íŠ¸ ìˆ˜: %d", len(all_texts))
    logger.info("  - ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: %dì", sum(len(t) for t in all_texts))

    # 4) ìš©ì–´ ì¶”ì¶œ/ë­í‚¹
    logger.info("ğŸ”¤ ìš©ì–´ ì¶”ì¶œ ë° ë­í‚¹ ì‹œì‘...")
    corpus_terms: List[str] = []
    for i, txt in enumerate(all_texts):
        extracted = _extract_terms(txt)
        corpus_terms.extend(extracted)
        if i < 3:  # ì²˜ìŒ 3ê°œ í…ìŠ¤íŠ¸ë§Œ ìƒì„¸ ë¡œê¹…
            logger.debug("  %dë²ˆ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ìš©ì–´: %s", i+1, extracted[:10])
    
    logger.info("ğŸ“Š ìš©ì–´ ì¶”ì¶œ ê²°ê³¼:")
    logger.info("  - ì¶”ì¶œëœ ìš©ì–´ ìˆ˜: %d", len(corpus_terms))
    logger.info("  - ê³ ìœ  ìš©ì–´ ìˆ˜: %d", len(set(corpus_terms)))
    
    trending_terms = _rank_terms(corpus_terms, top_k=8)
    logger.info("ğŸ† ë­í‚¹ëœ íŠ¸ë Œë”© ìš©ì–´:")
    for i, term in enumerate(trending_terms):
        logger.info("  %dìœ„: %s", i+1, term)

    # 5) ì‹œì¦Œ ìŠ¤íŒŒì´í¬ ê°ì§€(ê°„ë‹¨ í‚¤ì›Œë“œ ë§¤ì¹­)
    logger.info("ğŸ“… ì‹œì¦Œ ìŠ¤íŒŒì´í¬ ê°ì§€ ì¤‘...")
    seasonal_spikes = _guess_seasonal_spikes(all_texts)
    logger.info("ğŸ“… ë°œê²¬ëœ ì‹œì¦Œ ìŠ¤íŒŒì´í¬:")
    for spike in seasonal_spikes:
        logger.info("  - %s: %s", spike.get("term"), spike.get("window"))

    # 6) ë…¸íŠ¸(ê°„ë‹¨ ìš”ì•½)
    logger.info("ğŸ“ ë…¸íŠ¸ ìƒì„± ì¤‘...")
    if web_search_res and web_search_res.get("results"):
        note = f"ì›¹ ê²€ìƒ‰ ê²°ê³¼ {len(web_search_res['results'])}ê±´ ìˆ˜ì§‘"
        notes.append(note)
        logger.info("  - %s", note)
    if scraped_res and scraped_res.get("documents"):
        note = f"ìŠ¤í¬ë© ë¬¸ì„œ {len(scraped_res['documents'])}ê±´ ë¶„ì„"
        notes.append(note)
        logger.info("  - %s", note)
    if mk_res and mk_res.get("results") is not None:
        note = f"Supabase ë§ˆì¼€íŒ… ê²°ê³¼ {len(mk_res['results'])}ê±´"
        notes.append(note)
        logger.info("  - %s", note)
    if yt_res and yt_res.get("results") is not None:
        note = f"Supabase ë·°í‹° ìœ íŠœë²„ ê²°ê³¼ {len(yt_res['results'])}ê±´"
        notes.append(note)
        logger.info("  - %s", note)

    snapshot = {
        "trending_terms": trending_terms,
        "seasonal_spikes": seasonal_spikes,
        "notes": notes,
        "sources": sources[:max_results],
        # í•„ìš” ì‹œ ë””ë²„ê¹…ìš© rawë¥¼ ë„˜ê¸¸ ìˆ˜ ìˆìœ¼ë‚˜, ê¸°ë³¸ì€ ìƒëµ ê¶Œì¥
        # "raw": {
        #   "web_search": web_search_res,
        #   "scraped_pages": scraped_res,
        #   "marketing": mk_res,
        #   "youtuber": yt_res,
        # },
    }

    logger.info("ğŸ‰ ì§€ì‹ ìŠ¤ëƒ…ìƒ· ìƒì„± ì™„ë£Œ!")
    logger.info("ğŸ“Š ìµœì¢… ìŠ¤ëƒ…ìƒ· ìš”ì•½:")
    logger.info("  - íŠ¸ë Œë”© ìš©ì–´: %s", trending_terms)
    logger.info("  - ì‹œì¦Œ ìŠ¤íŒŒì´í¬: %s", seasonal_spikes)
    logger.info("  - ì†ŒìŠ¤ ìˆ˜: %d", len(sources[:max_results]))
    logger.info("  - ë…¸íŠ¸: %s", notes)
    
    return snapshot